---
title: "The *Ushichka* dataset: a multi-channel, multi-sensor dataset to unravel active sensing in groups"

author: "Thejasvi Beleyur, Holger R. Goerlitz"

date: "Document updated: `r Sys.Date()`" 

output: 
  bookdown::word_document2:
  fig_caption: yes
  pdf_document:
    extra_dependencies: ["babel","float"]
  latex_engine: 
    lualatex

abstract: The collective behaviour and motion of animal groups  has been intensely investigated. The behavioural heuristics underlying the impressive acts of self-organisation have been revealed through a combination of experimental tracking and modelling. More specifically, visually driven animal collectives such as fish and mammals have been the focus of studies. Visually driven collective behaviours scale well with group size given sufficient ambient light. In contrast to passive sensing animals, active sensing animals like echolocating bats emit probes of energy to detect their surroundings. Echolocating bats emit loud ultrasonic calls and listen for faint returning echoes to detect objects. When multiple bats echolocate together however, it is expected that they will not be able to detect their surroundings well due to masking. This masking means that collective behaviour in active sensing bats is not expected to scale well with group size. However, despite this expectation, bats aggregate and fly together in large groups during cave emergences and mating swarms. The sensory and behavioural heuristics behind these formations remain understudied to date. The study of group echolocation has also been impeded by technological issues in experimental tracking, analysis of recordings with high call densities, and a deficiency of parametrised computational models. To fill these gaps in the study of  active sensing groups, we present the *Ushichka* dataset. *Ushichka* is a multi-channel, multi-sensor dataset consisting of audio and video data of bats echolocating as they fly in a cave. A LiDAR scan of the recording volume is also part of the dataset. *Ushichka* promises to generate a detailed spatio-temporal understanding of how bats in groups deal with the problem of masking as they echolocate together in close proximity. The dataset is, to our knowledge, one of its kind in capturing bat collective behaviour with multiple sensors, and promises novel insights into the sensorimotor heuristics of active sensing groups. While promising novel insights, *Uschichka* also presents a series of technical challenges ripe for inter-disciplinary collaborations such as LiDAR-thermal camera alignment, multi-channel call correspondence matching and automatic microphone position estimation. The processed dataset will consist of individual level trajectory and call information that are aligned to the LiDAR scan of the cave. This processed dataset can be used to parametrise existing models of collective behaviour, compare behavioural heuristics with group size, and to reconstruct the auditory scene of each bat in the group. 

bibliography: ushichka_references.bib

---

```{r echo=FALSE, message=F, warning=F}
library(knitr)
```

#  Introduction

Many animals across the tree of life move and live in groups, and show impressive coordinated behaviours [@sumpter2006principles]. Coordinated behaviours shown in these collectives are reproduced by simple heuristics in models with agents responding independently to the movement and positions of their neighbours [@reynolds1987a;@vicsek1995;@couzin2002a]. These coordinated behaviours have been well investigated in visually driven animal collectives such as birds, mammals and fish [@ballerini2008a;@strandburg2013visual;@pita2016collective]. Vision is a 'passive' sensory modality, in that the animal acts solely as a receiver of light [@nelson2006a]. Visually driven collective behaviour can thus scale well with group size, given  sufficient ambient light.

In contrast to passive sensing animals, are 'active' sensing animals. Active sensing animals (*sensu stricto* [@nelson2006a]), such as bats, dolphins and electric fish, emit probes of energy and monitor the modulation of the probes by the environment around them . Echolocating bats emit loud ultrasonic calls, and listen to the returning echoes to detect objects around them [@Griffin1958]. When multiple echolocating bats echolocate together, they can start to negatively affect each other's ability to detect their own echoes [@ulanovsky2008bat]. Every bat emits loud calls, and is attempting to listen to its own echo. However, along with its own returning echoes, each bat hears loud 'non-target' sounds (calls and echoes of neighbouring bats) which hamper its own echo detection. A bat that cannot detect its own echoes is metaphorically 'flying blind', and risks colliding into its neighbours , and of more concern, into hard structures around it. As the number of bats in a group increases, the number of non-target sounds increases rapidly in a non-linear fashion (~$(N_{bats}-1)^2$). Sensory simulations show that in large groups of bats, a bat may be detecting its neighbours every few calls - and not every time it emits a call [@beleyur2019modeling]. 

Despite the expected deterioration of echolocation with increasing group sizes, echolocating bats in nature are social and form very large aggregations in the form of mating swarms, cave roosts and evening emergences [@Ortega2016;@Erkert1982;@gillam2010tbrasiliensis]. The dynamics of active sensing groups is understudied [but see @theriault2010a], and perhaps likely to differ from passive sensing groups in one respect. Given sufficient light, an animal in a visually driven collective will be able to detect all neighbours in its vicinity even at large group sizes. In contrast, the number of neighbours potentially detected by a bat in a group decreases with group size [@beleyur2019modeling], leading to a limited sensing ability. Computational modelling of groups [@bode2011a] with 'limited interactions' (where one neighbour is detected at a given time, and detection is proportional to proximity) are able to recreate collective behaviours.   However, it remains to be experimentally seen in what respects the behaviour of active sensing groups differ from the visually driven animal collectives studied so far. Even if active sensing groups are expected to behave much like visually driven passive sensing groups, experimental confirmation remains lacking to date.

Here we present the conceptual motivation and technical equipment behind the *Ushichka* dataset. The word *Ushichka*  (Ушичка, *OO-shi-ch-kaa*) in Bulgarian has a dimunitive connotation for something with multiple ears. The name is chosen to highlight the fact that *Ushichka* is a  multichannel, multi-sensor dataset which aims to unobtrusively 'listen' (observe with technological eyes and ears) to groups of multiple bats echolocating in the wild. 

The *Ushichka* dataset  is, to our knowledge, a unique dataset in that it comprises of multichannel audio and video datasets of bats echolocating in groups in their *natural environment* captured by a LiDAR scan of the space. The multichannel nature of the dataset allows 3D localisation and tracking with high spatio-temporal resolution. Previous experimental work on multi-bat echolocation in the lab and field to date has mostly been with up to 2 bats a time [@jones1994individual;@goetze2016a;@giuggioli2015a;@necknig2011between;@fawcett2015clutter;@fawcett2015echolocation], or  multichannel data of solely video or audio [@theriault2010a;@lin2016bats]. The multi-camera, multi-mic nature of *Ushichka* allows us to explore echolocation in groups in much greater detail than previously attempted. 

## The experimental study of echolocating bat groups

The study of bat echolocation has a long and grand history dating back over 200 years, first documented in Spallanzi's experiments in the 18th century.
After the formal discovery of echolocation [@Griffin1958;@Dijkgraaf1946], the field has moved leaps and bounds. A detailed understanding of the physiology, behaviour, neurobiology and acoustics behind echolocation has been uncovered in great detail [@popper1995hearing;@Fenton2016]. While a large body of knowledge is in place about how *individual bats* are able to echolocate through the use of trained animals in laboratory settings and field observations [@Fenton2016], comparitively little is known on how they echolocate in groups [@ulanovsky2008bat].

Previously, the challenges in echolocation fieldwork included bulky analog recording and signal analysis equipment; limiting the speed and quantity of data that could be collected, and analysed [@Griffin1958;@habersetzer1981a]. The onset of the digital revolution has allowed for lightweight devices that can collect more data, which can then be analysed more easily. Current experimental approaches to study echolocation in groups include, for example, on-body tags that record audio from the focal animal and its neighbours (along with GPS and accelerometer data)[@cvikel2015b], microphone-camera pairs to quantify broad echolocation characteristics with group size [@lin2016bats] and thermal camera 3D tracking with sophisticated computer vision to track individuals in the million-strong emergence behaviour of *Tadarida brasiliensis* [@betke2007tracking;@theriault2010a]. Each of these studies have provided exciting and important insights into how echolocating bats manage to echolocate in groups. For instance, @cvikel2015b showed that *Rhinopoma microphyllum* do not seem to show the 'jamming avoidance response' (shifting call frequencies to avoid  overlap with neighbours, sensu @ulanovsky2004dynamics) first seen by @habersetzer1981a in its congener *R. hardwickei*. @lin2016bats find that *Miniopterus fuliginosus* bats echolocating in caves actually increase their pulse rate with group size, in contrast to theoretical expectations and results from *T. brasiliensis* in captivity which decrease their pulse rate [@jarvis2013groups]. @theriault2010a advanced the field by quantifying the geometry of a *T. brasiliensis* emergence, showing that animals in larger groups fly relatively close to each other (0.5\ m), and that neighbours are placed uniformly across azimuth and elevation around individuals, unlike that seen in bird flocks [@ballerini2008a]. In starling flocks for instance, neighbours are located non-uniformly in the elevation and azimuth in that they are less likely to be found in front or behind a focal individual. 

*Ushichka* represents a natural and quantitative step forward to all the recent work in the study of group echolocation in the field. The one important respect where *Ushichka* differs from previous work is its inherent 'multi-modality', in that it consists of simultaneously captured multi-microphone, multi-camera data streams, along with 3D scans of the recording volume for physical context. On-body tags and single microphone recordings lack explicit information of neighbour location and how many neighbours are there around the focal bat. Pure video tracking with one or more cameras does not provide insights into the echolocation behaviour of animals across group sizes. *Ushichka* captures flight and echolocation data of the group, and holds promise to provide access to the auditory scene [@Moss2001] of each individual in the group. Reconstructing the sensory inputs of individuals in a group allows us to better understand the sensorimotor decisions animals make [@strandburg2013visual]. While an established method in visually driven animal groups like fish, auditory scene reconstruction is an exciting new frontier for the field of group echolocation, that *Ushichka* promises to provide. 


#  Experimental methods

## Field site and study species 
Recordings of wild echolocating bats were made in the Orlova Chuka cave system (Ruse, Bulgaria). Recordings were made of bat flight and echolocation in the chamber first encountered after the entry corridor (Figures \@ref(fig:cavesetup), \@ref(fig:cavesetup2)). Recordings were conducted in the recording volume (*w x l x h*~5x9x3m$^{3}$) between 19th June to 19th August 2018 over the course of 14 recording sessions (Suplementary Information (SI): Table 6.1). One recording session typically started around sunset and ended around sunrise the next day (See \@ref(recschedule)).

Orlova Chuka is known to host at least 9 species of bats [@Ivanova2005;@govtbatcount]. During the summer, resident  *Myotis myotis* and *Myotis blythii* females form the dominant bat population. The *M. myotis* and *M. blythii* (hereon referred to as *M. myotis/blythii*) female bats arrive in spring and spend the summer in the cave system raising their young. Though rigorous mark-recapture data is missing , individual mothers are known to be consistent in returning to the cave system over multiple nights (pers comm, Laura Stidsholt, Stefan Greif). Given the numerical dominance of  *M. myotis/blythii* in the cave system, it can be fairly said that the *Ushichka* dataset consists mainly of these two species. The calls of *M. myotis* and *M. blythii* are indistinguishable (their morphology is also extremely similar too) [@dietz2016bats]. Due to the great similarity in their echolocation calls, they are treated as one group of bats.  The frequency ranges of *M. myotis/blythii* are lower than those of the other resident frequency modulating bats in the cave (eg. *M. daubentonii, M. capacinii, Miniopterus schreibersii*) and are very easily distinguishable from the rhinolophid bats (*Rhinolophus ferrumequinum, R.euryale, R. mehelyi*)[@dietz2016bats]. 

```{r echo=FALSE}
# Bat flight and echolocation were recorded in the same recording volume across a period of two/three months with more or less the same recording setup (3 thermal cameras, 12-22 microphones). During both morning and evening recording sessions, bats typically flew a few rounds in the recording volume (*w x l x h*~5x9x3m$^{3}$) before heading back into the direction they came from, or flying into another direction. Recordings were conducted 
```

```{r cavesetup, echo=FALSE, fig.cap='The *Ushichka* setup. The large inverted T shaped array in the centre of the image is the 120cm tristar array with four SANKEN CO-100 microphones. The three blue lights are from the three thermal cameras pointing towards the array. The rest of the microphones are smaller Knowles SPU 0410 units are attached to the wall on the left portion of the image and are not visible.'}
cavesetupimg = 'figures/ushichka_setup_DC6A5930_w.JPG'
include_graphics(cavesetupimg)
```

```{r cavesetup2, echo=FALSE, fig.cap='The *Ushichka* setup with the smaller Knowles SPU microphones on the wall referred to in Figure \\@ref(fig:cavesetup). In the picture is a *M. myotis/blythii* bat flying past the array. Photo by Stefan Greif.'}
cavesetupimg2 = 'figures/DC6A6061.JPG'
include_graphics(cavesetupimg2)
```


## Audio, video and LiDAR systems 

### Audio  

Multichannel audio data was recorded by multiple (2-3) Fireface (RME GmbH, Germany) sound cards running at 192kHz (Figure \@ref(fig:multichannel-calls)) connected to a laptop (DELL Latitude E6540, Windows 7 enterprise, Intel Core i5, 16GB RAM). The laptop was verified not to emit ultrasound. All sound cards were Fireface USB type, ASIO protocol based soundcards. The number of channels of audio in the *Ushichka* dataset varied between 12-22 channels over the course of the field season. The 12-22 channel data consisted of four Sanken CO-100 (Sanken, Japan) microphones with the rest of the 8-18 channels coming from Knowles SPU-0410 (Knowles, USA) microphones. All microphones were connected to either the inbuilt pre-amplifiers on the sound cards, or to two RME QuadMic II (RME GmbH, Germany) pre-amplifiers. Later recordings with the *Ushichka* setup also included a Focusrite Scarlett OctoPre (Focusrite Plc, UK) pre-amplifier. 

All recording nights had a 4-channel 120cm tristar mounted in a fairly constant position in front of the memorial plaque in the recording volume. A 'tristar' array is a common planar array configuration [@Hugel2017;@Goerlitz2010;@Lewanzik2018], with three microphones placed at a fixed radius from a central microphone. The three peripheral microphones are separated from each other by a 120 degree angular separation. The entire set of four microphones is accomodated on an inverted T-shaped metal array. The Knowles SPU-0410 microphones were spread around the recording volume to increase the variety of locations at which bat calls were recorded. The positions of microphone were constant within a recording session, and were often changed between sessions. On a few occasions, the microphones were kept in the same position across consecutive recording nights. Microphones were not left in the cave across multiple days due to concerns of humidity affecting the electronic circuits and membranes, especially of moisture build up in the Knowles SPU 0410 recording inlet. 

A subset of all possible inter-mic distances were measured every recording session using a GLM 50C (1mm accuracy) laser range finder (Bosch, Germany). Ultrasonic sweeps were played back from multiple points in the recording volume as part of an  automatic mic position estimation workflow. Most of the microphones were calibrated for their frequency response and directionality at least once after the field season. For more information on the microphone automatic position estimation and calibrations please refer to the Section \@ref(noframes) and the SI. 

### Video  

Three thermal cameras (9mm focal length, TeAx ThermalCapture 2.0, TeAx Technology GmbH, Germany) running with FLIR Tau 640 cores (640x512 pixel image resolution) at 25Hz formed the camera array. All three cameras (Figure \@ref(fig:multicambats)) were frame synchronised. There were a few occasions where one or two cameras recorded an extra frame, and these extra frames are known from previous tests to be at the end of the recording, and not jitter in recording initiation. 

```{r multicambats, echo=FALSE, out.width="100%",fig.cap="A multi-camera view of bats flying in the recording volume, allowing 3D tracking of flying bats (from the frame-synchronised thermal video recordings)."}
multicambatsimg = 'figures/multicam_views.png'
include_graphics(multicambatsimg)
```

The microSD cards (SAMSUNG PRO+, 32 GB) were inserted into each camera at the start of each recording session. Removing the microSD card from the cameras required significant disturbance of their position, and thus the total video recording time was limited by the one-time microSD card memory capacity. The location of the microSD cards *inside* the camera housing meant that the data from the cameras could only be transferred in the morning after the end of the recording session. All cameras were calibrated using the 'wand' protocol described in @Theriault2014. Further details on camera intrinsic and extrinsic parameter calibrations, wand setup, and synchronisation with audio are in the SI. 

### LiDAR 
A LiDAR scan of the recording volume provides a physical context to place the observed echolocation and flight behaviour of the bats. With only the audio and video data, we will at most be able to recreate the relative positions and time of call emissions of the bats themselves. On aligning the acoustic and video tracking coordinate systems with the LiDAR scan, we will have access to the *absolute* position of the bats in the cave. While bats are likely reacting to each other's presence, they will also be reacting to the presence or absence of physical obstructions such as cave walls. With the  LiDAR scan for example, we will be able to understand if a change in trajectory or call behaviour arose from the bats' proximity to the cave wall, or to its nearest neighbour. 

A LiDAR scan of the recording volume, along with a large portion of the Orlova Chuka cave was carried out in collaboration with Dr. Asparuh Kamburov (University of Mining and Geology “St. Ivan Rilski” , Sofia). The LiDAR scan provides a high spatial resolution (<1cm) 3D map of the cave's surface and objects in it (Figure \@ref(fig:lidarimage)). The details of the LiDAR scanning, data processing and description can be found in @bggeospatial. 

```{r lidarimage, echo=FALSE, fig.cap='The LiDAR map of the recording volume showing the same view as in Figure \\@ref(fig:multicambats), with the large tristar array.'}
lidar_scan_img = 'figures/lidar_image.png'
include_graphics(lidar_scan_img)
```

### Weather data
The local weather conditions (humidity, temperature and atmospheric pressure) within the cave were recorded continuously by a Kestrel 4000 weather meter (Nielsen-Kellerman, USA) in the recording volume, typically hung from the tristar array stand. The logged weather variables play a role in the accuracy of position and source level estimates [@goerlitz2018weather;@lawrence1982measurements], and were thus measured to later estimate the speed of sound and absorption of ultrasonic sound more accurately across recording sessions. 

## Recording schedule {#recschedule}
The audio and video arrays were mostly setup before sunset (~21:00 Eastern European Time) and collected data till around sunrise (~5:30 EET) with one break in between. The recording break between around 12-3 am coincided with an observed drop in bat activity around midnight. 

Recordings were triggered manually in 2 of 14 recording sessions in response to bat calls heard on a bat detector (SI Table 1). For the rest of the sessions, recordings were triggered automatically. A recording was automatically triggered by bat calls that went above a threshold rms level across a subset of monitor channels (between -50 to -40 dB rms, threshold adjusted across recording sessions). Once triggered, the recording was set to stop after a pre-defined duration. There were no other sources of ultrasonic sound in the cave, and it can be said with great certainty that all recordings were triggered only by bat calls. Each recording in the *Ushichka* dataset consists of 10-15 second long audio-video file pairs (See SI for details of audio-video synchronisation). The variation in recording duration is due to adjustments made over the course of the field season. 

Bats can emit over 10 calls per second, which would lead to an almost continuously triggered recording system. While technically feasible, continuous recording of data would lead to unmanageably large files. Moreover, the TeAX thermal cameras relied on high speed microSD cards that were the limiting factor in the amount of data (32 GB) that could be collected over the course of one recording session.  The audio-video recording system was thus set to record at a fixed duty cycle of around 20% across sessions. For example, a 20% duty cycle and 10 second recording duration meant that recordings could be triggered at most every 50 seconds (10 seconds recording followed by 40 seconds inactivity). 

```{r multichannel-calls, echo=FALSE, fig.cap="Spectrograms showing a subset channels of four channels from the multichannel audio data collected. The spectrograms show highlight some of the challenges of working with the audio data including reverberation, multi-path propagation, call overlaps, and correct call identity matching across channels. Each column shows the 4 channels corresponding to one recording. Column A) rows 1-4: an example of a recording with one bat echolocating. Column B) rows 1-4: recording with multiple bats in it. Each"}
multichcalls = 'figures/single_multibat_specgram.png'# 'figures/multichannel_composite_elongated.png' #'figures/multichannel_composite.png'
include_graphics(multichcalls)
```

There are a few occasions of skipped morning and evening recordings due to various technical and logistical issues that were encountered over the course of the field season.

## Bat activity
Bats began to fly from the interior roosting sites into the recording volume around sunset. The initial activity is typically due to of small groups (2-5) of *R.euryale* and *R. mehelyi* flying rapidly and unpredictably in the volume. The activity of the rhinolophid bats seemed to reduce with time, and gave way to a slowly rising number of *M. myotis/blythii* bats. The number of bats flying in the recording volume could be as high as thirty bats at a time. Towards end June, it was noticed that the *M. myotis/blythii* bats also formed temporary roosting clusters on the walls of the recording volume, and this behaviour seemed to decrease in its frequency as the season progressed. Cloudy and rainy evenings seemed to lead to a delay or complete absence of emergence from the cave. 

Peak bat activity in the recording volume was typically seen about 1-1.5 hours after sun-set. The bats then began to exit the cave (Figure \@ref(fig:activity)). The exit seemed relatively rapid, after which only a few bats were seen to still fly in the recording volume. Individual bats returned in the early morning from one to two hours before sunset onwards. It seemed that *R. euryale* and *R. mehelyi* activity remained fairly constant through the night, even during the drop in activity around midnight. 

```{r activity, echo=FALSE, fig.cap='Schematic of typical bat activity pattern of Myotis myotis and Myotis blythii between sunset to sunrise in the *Ushichka* dataset. The multichannel recording system was typically activated before sunset and was run till after sunrise with a break in between'}
activityimg= 'figures/ushichka_activity.png'
include_graphics(activityimg)
```

## Software used in this work
Automatic recording triggering implemented in the ```fieldrecorder_trigger``` module relies on the open-source Python language [@van2011python], and the sounddevice, scipy and numpy [@geier2015a;@virtanen2019a;@oliphant2006a] packages. This manuscript was written using the knitr package [@knitr]. 

# Technical challenges ahead and potential solutions in analysing the *Ushichka* dataset

Analysing the echolocation call data in *Uschichka* will require surmounting a series of challenges, here discussed in increasing order of difficulty.

1) Reflections : The recording volume consisted of acoustically reflective surfaces, leading to significant multi-path propagation. Multi-path propagation occurs when a sound arrives multiple times at a microphone due to distinct reflections. For instance, a call emitted by a bat reaches the microphone first, and is then followed by the reflection from the ground, cave wall and the ceiling. Multi-path propagation is likely to be somewhat difficult to deal with, especially considering that if a single call leads to *N* multi-path reflections, then *M* calls in the volume could then lead to *NxM* reflections. The many false 'copies' of the original sound within each channel creates spurious peaks in the cross-correlation between two channels. Solutions to handling multi-path propagation exist, such as the DATEMM algorithm and its variants [@scheuing2008disambiguation;@zannini2010improved].  DATEMM proposes a method based on estimating within-channel multi-path propagation using auto-correlation and identifying reliable cross-channel time delay estimates by choosing peaks that represent direct paths. 

2) Call density : Each bat flying in and out of the volume emits calls 10-20 times per second (10-20 Hz call rate). With one bat, it is relatively straightforward to acoustically localise the bat as calls are separated by tens of milliseconds. With an increasing number of bats in the volume, (as happens towards the end of the night, Figure \@ref(fig:activity)), the temporal separation between calls will decrease, and also lead to a higher occurence of call overlaps. Handling such high call densities will be a challenge, and require the latest techniques in call detection in the presence of overlaps [@izadi2020separation], or in general blind signal separation techniques [@brandstein2013microphone]. 

3) Correspondence matching : Even if calls can be detected reliably in single channels, the problem of cross-channel correspondence still remains to be solved, a common problem faced in any multi-channel acoustic array [@brandstein2013microphone]. An echolocation call detected in one channel needs to be matched with itself in another channel, despite the possible presence of multiple similar calls in the expected time-window. Individual bats do have unique call characteristics (@masters1995sonar, and in *M. myotis*: @yovel2009voice; but also see @siemersnovoice). The unique echolocation call 'signatures' are sometimes apparent from visual inspection of the spectrogram. However, manual correspondence matching is unlikely to scale well in terms of effort required with group sizes beyond 3 bats and will require robust automated algorithms to solve this problem. Here too, DATEMM-type algorithms [@scheuing2008disambiguation] may present a solution. Much like multi-path propagation, the presence of overlapping sounds also creates spurious peaks in cross-correlations between two channels. DATEMM is able to handle and localise overlapping sounds by filtering cross-correlation peaks based on the assumptions that follow from sources that have direct path propagation. While the output of a DATEMM type localisation is the 3D positions of the sources, these sources correspond to a set of time-difference of arrivals (TDOAs) across channels. Knowing the candidate TDOAs may allow post-hoc matching of the emitted calls across channels. 

One important 'support system' which may significantly help in solving the issues of multi-path propagation and correspondence matching is the multi-camera video data. Each call is emitted from one position by a bat, and captured on multiple audio channels. The time-synchronised 3D video tracking data contains the microphone positions and bat trajectories. With this in mind, we can combine the audio and video tracking to simplify cross-channel correspondence matching and the process of assigning a call to its source bat.  The basic principle of acoustic localisation relies on the fact that with at least 4 microphones spread in space, each position of sound emission will generate a unique set of time-difference-of-arrival (TDOA) . A TDOA in a microphone array is the difference in sound arrival times with reference to a chosen reference channel. Measuring TDOAs allows inferring the position at which a bat emitted a call from. The 3D video trajectory of a bat in the cave present all possible points of call emission.  Calculating the distances between a bat and each microphone across time will allow us to generate a set of 'predicted' TDOAs that calls could be arriving at. The problem thus boils down to a matching problem, where given a set of bat flight trajectories we are able to rule out a larger number of spurious TDOAs. The trajectory based filtering approach could in principle elegantly solve the problems of multi-path propagation and cross-channel correspondence at one shot. However, whether the 3D video data will actually be able to deliver sufficiently accurate position estimates, and thus accurate TDOAs remains to be seen.  


## Fusing multiple data streams to reconstruct individual auditory scenes
Reconstructing individual auditory scenes requires recreating the incoming sounds (echoes, neighbour's calls) and outgoing sounds (emitted calls) experienced by each bat in a group. Emitted sounds can be assigned through acoustic tracking of calls using the microphone array. The incoming echoes a bat hears in the recording volume will come from targets such as other bats and the walls of the cave. The incoming calls will be from the calls emitted by neighbouring bats. Recreating the incoming sounds heard by a bat thus requires knowing the relative positions of the bats in the group and their locations in the recording volume. Reconstructing auditory scenes in the group thus requires a fusion of the various data streams in *Ushichka*, along with their alignment into a common coordinate system.

The audio and video streams are tightly linked as they describe the flight and echolocation behaviour of the bats in the recording volume. However, the bats in the recording volume are likely trying to mainly avoid collisions with the walls and structures in the cave while the location of other bats is probably of lesser concern. This is where the LiDAR dataset informs the how and why of individual behaviours in the cave. Fusing the acoustic and video data streams is relatively straightforward in that they are time-synchronised and they both track common 'objects' (bats). The 3D positions derived from both data streams can be overlaid to match with a simple rigid rotation of coordinate systems. To understand the bats' echolocation and flight in their physical context however, the LiDAR scan of the recording volume must be aligned with the coordinate systems of the camera array. This will allow us to interpret the sensory decisions of bats correctly, eg. whether a sudden turn was due to proximity to the cave wall. 

Recent advances in the alignment of LiDAR scans with camera views have led to the formulation of 'targetless' workflows that do not require specific calibration objects for alignment [@kang2020automatic;@munozbanon2020]. These approaches rely on recreating synthetic camera images by projecting the 3D LiDAR scan into 2D (eg. Figure \@ref(fig:multicambats)). The best LiDAR-camera alignment is one which which achieves the highest similarity between the synthetic 2D projection and observed camera image (or some transformation of the images). *Ushichka* is fit to use the alignment methods mentioned above as the extrinsic (position and rotation) and intrinsic (eg. focal length, distortion) camera parameters required for the 2D projection are known for each recording session. One potential issue that may arise is that the methods of  @kang2020automatic and @munozbanon2020 have been developed for visible light cameras. Thermal camera images often look very different from light camera images, and may require different processing steps. To our limited knowledge current thermal camera-LiDAR alignment protocols require a specifically designed calibration object [@krishnan2017cross;@slatcalib;@zhangcalib], and  there is limited work on target-less alignment of LiDAR and thermal data [@phuc2017registration]. While it may be possible to use the target-less workflows for *Ushichka*, the lack of thermal dynamic range in images may present an additional challenge. In contrast to RGB images, thermal images are mono-channel and the cameras we used also have a limited thermal resolution (~1$^{\circ}$ C). In addition to device limitations, the actual thermal environment in the cave may pose a challenge. The cave system maintains a relatively stable temperature of around 10$^{\circ}$ C throughout the year. From a thermal perspective, the constrast between one part of the cave and another may not be as large as what is seen in many RGB images of a given view. Despite these challenges, one advantage *Uschichka* may have is that each recording session has *three* camera views (instead of one) that can be used to align the LiDAR scan. In this respect, our three camera situation may result in a more constrained alignment despite the poor thermal gradient and device limitations in the thermal images. 

## Automatic mic position estimation removes the need for bulky frames {#noframes}

Working with multi-microphone arrays typically includes carrying many long cables and heavy frames to hold microphones in known positions. Microphone positions need to be known for successful acoustic localisation, and precise mic positions lead to lower localisation errors [@Wahlberg1999].  However, carrying large array frames into the field, especially in cluttered environments such as caves or forests is often difficult and time-consuming. In response to these novel conspicuous objects, bats may actually focus their calls specifically on the frames and the recorded behaviour might actually be an unwanted artifact of the setup itself. There is thus a pressing need in bioacoustics for the development of acoustic arrays that are less conspicuous, and can be setup universally and quickly. One option, as seen in *Ushichka*, is to place the microphones on existing structures such as cave walls and rocks (branches, signposts and other structures may also be useful in other field sites). Using existing structures to hold microphones does not generate new obstacles or points of interest, and is thus less likely to generate artifactual inspection behaviours from the animal. 

Once frames have been abandoned in favour of existing structures, the task of measuring microphone positions still remains. The most accurate solution to directly measure microphone positions  is to use a Total station theodolite. Total station's have measurement acccuracies of a few mm, and directly output the 3D positions of microphones. A major issue with the use of a Total station is the addition of yet another piece of equipment, and the time taken to set up the device itself for measurements. Alternately, it is possible to estimate mic positions indirectly by exhaustively measuring inter-mic distances using a laser range finder or tape-measure (as has been done for *Ushichka*). However, experience has shown manual inter-mic measurements can be error-prone at larger distances (~4-5m) with a laser range finder due to jittery hands, and become impractical with tape-measures. Manual inter-mic distance measurements require direct line-of-sight between microphones and do not scale well with an increasing number of microphones in an array (number of measurements to be made scales rapidly with $\frac{N_{mics}\times N_{mics}-1}{2}$).  

Given the problems around microphone position measurements without frames, we were looking for solutions that have the ease of today's camera calibration workflows [eg. @Theriault2014], and found the 'Structure-from-Sound' approach of @zhayida2016automatic. @zhayida2016automatic were able to infer microphone positions based only on commonly recorded playbacks, and without the need for any actual inter-mic distance or position measurements. We were also able to verify the utility of the Structure-from-Sound approach for our freely-placed microphones on ground-truthed data [@sfs_cotdoa, also see SI]. Such automatic position estimation workflows have the potential to drastically reduce the effort needed to run multi-channel recordings in the field, and open up the exciting realm of inconspicuous experimental setups. We are currently continuing the collaboration and looking to formulate mic position estimation workflows that are field-friendly and widely applicable across diverse field-sites.

# Investigative opportunities opened by the *Ushichka* dataset

Here we detail some of the possible lines of investigations that can be undertaken with *Ushichka* once the necessary analytical workflows are in place. 

## Collective behaviour in active sensing groups 
The combination of acoustic and video tracking along with a LiDAR scan of the cave provides us sufficient data to reconstruct the sensory inputs of each bat in a group. To reconstruct the sensory inputs of each bat, we would require baseline data on a series of parameters such as call and hearing directionality, auditory masking, hearing threshold and source level. These parameters can either be extracted from the literature (as done by @beleyur2019modeling and @mazar2020sensorimotor) or measured using the acoustic tracking system itself (eg. source level and call directionality). Given a group of *N* bats, we can thus reconstruct all the sounds each bat would have heard in their inter-pulse intervals (the silent gap between calls). Previous studies have attempted to simulate such contexts with biologically parametrised values of the input sounds [@beleyur2019modeling;@mazar2020sensorimotor]. Our experimental measurements in combination with detailed sound propagation models could allow for *direct* sensory input reconstructions in a whole group of bats simultaneously. Knowing the sensory inputs animals receive over time would allow us to finely parametrise the relevant models of collective behaviour and test how well they predict group  motion. For instance, @bode2011a model a group of agents that perform non-simultaneous and discontinuous 'updates' of their environment. Their model is very similar to how groups of bats get information about their surroundings, as each bat is very likely emitting calls independently [@hase2018a]. @bode2011a moreover model 'limited-interactions' in collective motion, where each agent can only detect the positions of one neighbour  at each update, very similar to what bats may be experiencing in large groups [@beleyur2019modeling]. Using the 'limited-interactions' model, @bode2011a are able to show that their predictions match empirical observations of starling flock structure. More importantly, @bode2011a also observe that their 'limited-interactions' model results in quantitatively different group structure in comparison to commonly used zone-based models [eg. @couzin2002a]of collective behaviour. The @bode2011a model can be parametrised by estimating how often bats call in a group (update rate), and how many neighbours they are typically able to detect (eg. using the framework in @beleyur2019modeling). The motion of *in silico* active sensing groups can then be compared with observed bat groups. Sensory reconstruction approaches in echolocation would also open the field to fine-scale temporal analyses of echolocation and flight behaviour: do bats turn away more often in response to an echo from a wall, or from the direction of a calling neighbour? Do bats call more often when they may have 'missed' echo detections due to call overlaps in the past few interpulse intervals?

## Optimising echolocation and flight strategies under challenging conditions
Bats are known to alter their echolocation rapidly according to the behavioural context, and ambient soundscape [@corcoran2017sensing]. *Ushichka* offers direct access to bat echolocation strategies as they are  acoustically challenged by increasing group sizes (Fig \@ref(fig:activity)). Aside from call-level data such as source-level, duration and spectral properties - the presence of multiple microphones spread across the room also provides access to call direction and beam shape modulations (Figure \@ref(fig:beamshape)A). While other call parameters have been investigated in studies with multiple animals, call directionality and beam-shape measurements have so far mainly been done with single animals in the field and lab [@surlykke2012a;@giuggioli2015a]. Revealing where bats aim their calls, and how they spread the energy in each call will uncover the sensory priorities bats may have while flying under challenging conditions. Do bats choose to focus narrowly on fast moving conspecifics, but broadly onto bigger obstructions like walls? Does their call direction and beam shape change with increasing group size? 

```{r beamshape, echo=FALSE, fig.cap="The benefits of multi-mic arrays for the study of echolocation and voice recognition (microphone: blue circle with line attached) A) Biosonar beam-shape can be estimated from microphones spread around the animals B,C) As the bat moves through the volume (C 1-8), the amplitude and spectral properties of the call received by the focal microphone (red) changes, as seen on the spectrogram (B). Having multiple microphones provides a multi-view picture of a call from many directions, allowing a better assessment of intra-individual call variation required for robust voice recognition"}
beamshapeimg = 'figures/beamshape_cleaned.png'
include_graphics(beamshapeimg)
```

The echolocation strategies in use may also inform why certain types of apparently stereotypical flight paths are seen within the recording volume. It was observed that bats sometimes tend to fly a circular loop close to the edges of the recording volume before leaving the cave or heading back into the roosting site (Figure \@ref(fig:loopingsch)). This 'looping' flight behaviour was observed in the evening flight activity as bats began to accumulate in the recording volume, but also when bats returned alone in the morning. Bats are known to show  stereotypic flight behaviours in the lab and in the wild [@mohres1949versuche;@barchi2013spatial], and this may perhaps be due to their limited attention or spatial memory.  However, in a cave setting, where there are no obvious obstacles, what drives the formation of these looping behaviours? Are the loops a stereotypic trajectory that allows the bat to limit its attention only along that route? Why do bats not take the shortest path and fly from the exit straight into roosting site? Initial observations of group flights suggested that even when multiple bats were flying together, not all parts of the recording volume were being 'used' by the bats. Some regions of the recording volume (regions close to walls)  seemed to have a higher density of bats. Is this wall-clinging behaviour a behavioural adaptation to maintain a high received level of wall-echoes (in the presence of non-target sounds)? The wall-clinging behaviour is somewhat reminiscent of structure-following behaviour seen in the field with horsehoe bats, that follow hedgerows instead of taking shortcuts over open fields. Horseshoe bats however, are likely to follow structures due to the limited sensing range from their high-frequency echolocation - why would myotids also adopt this behaviour? Another interesting parameter to estimate is the dynamics of the loop's chirality (clockwise/counter-clockwise) with group size. Alignment is a standard measure of how similar animals are in their direction of movement. When there are multiple animals in the recording volume, a measure of the alignment over time and group size might reveal whether the looping behaviour is an individually driven behaviour or is modulated by the flight directions of group members. 


```{r loopingsch, echo=FALSE, fig.cap="Schematic showing a view of the direct and looped flight paths as seen from above. The direct (broken blue line) and indirect (continuous green line) paths are flown in both directions (exit/entrance-cave interior, cave interior-exit/entrance). The grey quadrilateral outlines the recording volume."}
loopingimg = 'figures/looping.png'
include_graphics(loopingimg)
```

## Voice recognition for short-term and long-term behavioural tracking 
Many vocalising vertebrates across the animal kingdom have individually identifiable vocalisations, or 'voices' [@carlson2020individual]. The voice of each animal is a temporally stable cue, unlike experimental markings which may be lost over time. Despite the known stability of voice, for some animals one of the main challenges with developing a stable voice identifier is the variation in the vocalisation itself. Bird call repertoires can change over time, and complicate identification [@stowell2019automatic]. Approaches to date rely on machine-learning and statistical models, which require a training dataset of example vocalisations to 'learn' the patterns in the data. Variation caused by sound directionality, distance and ambient noise may often necessitate larger training datasets, which are often hard to obtain [@stowell2019automatic]. Unlike many types of animal vocalisations (eg. bird song, mammal social vocalisations), the spectro-temporal structure of individual bat echolocation calls are stereotypical and likely to remain stable because they serve a single sensory purpose – to detect objects. The dedicated sensory functionality of echolocation calls, and the general absence of background ultrasonic noise in the wild makes bat calls a convenient system to develop voice identification workflows.

Bat echolocation calls can be assigned to individuals by their acoustic features, though this has typically been done in small lab populations. Previous work [@masters1995sonar;@yovel2009voice] solved the 'closed-set' problem where all animal identities are known. *Ushichka* provides a large dataset over multiple nights and animals that fly by [also see Figure \@ref(fig:beamshape)B,C]. In contrast to the 'closed-set' problems solved in the past, *Ushichka* could be used to generate methods to solve the 'open-set' voice recognition task. Open-set classification problems deal with models that uniquely identify individuals that they have been trained on, but also those they have *not* been explicitly trained on. Even though the identities of bats echolocating in the recordings are unknown, it may be possible to exploit the behavioural patterns of the animals to approximate identity. Bats always return alone in the morning, and these recordings contain multiple echolocation calls from one individual as it enters the recording volume and heads away to the roosting site. In addition, it is known that a stable population of resident female *M. myotis/blythii* roost in the cave system over the entire summer period. Using these two facts, if successful, a voice recognition model should at least show certain patterns:  1) correctly assign identity to an independent subset of calls it was not trained on (validation data) from morning recordings 2) identify at least some individuals consistently across multiple morning returns, and in the best case 3) be able to pick up the individuals as they begin their activity in the recording volume around sunset. Of course, even if a trained model satisfies the three criteria above, it does not directly follow that the detections in scenarios 2) and 3) are not false positives. It may be necessary to systematically collect a larger sample of echolocation calls from hand-released individuals. The state of signal-processing and machine learning know-how in the field of human voice recognition is relatively mature at this point, with the use of speaker-specific Gaussian Mixture Models compared to Universal Background Models trained against multiple individuals [@Hennebert2009]. *Ushichka* may provide a test-dataset to examine how well the latest voice recognition approaches perform for echolocation calls. 

While seemingly challenging, if a validated voice recognition model is indeed developed successfully, it will allow revolutionary markerless short-term and long-term tracking of individual behaviours. Short-term behaviours such as flight and echolocation decisions over the course of a few seconds can provide insights into  individual sensorimotor strategies and the tracking of social interactions. Long-term decisions such as the change of evening cave exit-time and morning return times can reveal the changing physiological status of the resident females as their pups mature. Voice based individual recognition brings the same advantages gained by  tracking mammals  with unique visible markings (eg. zebras, tigers) [@lahiri11_biometric;@hiby2009tiger], allowing unobtrusive observations over multiple time-points and scales.

# Supplementary code and website

The list of supplementary code, data and information are below:

#. The ```fieldrecorder_trigger``` module used to trigger automatic recording is uploaded at the following repository:[https://github.com/thejasvibr/fieldrecorder](https://github.com/thejasvibr/fieldrecorder).

#. The speaker playback data to benchmark automatic microphone self-positioning is uploaded here: [https://doi.org/10.5281/zenodo.5126695](https://doi.org/10.5281/zenodo.5126695)

#. Those interested in viewing more snippets of the the *Ushichka* dataset and the progress of research related to it,  may wish to visit [https://thejasvibr.github.io/ushichka/](https://thejasvibr.github.io/ushichka/).

# Field work permits 
All field work at the Orlova Chuka cave was done with permission from the relevant authorities. *Ushichka* is a purely observational dataset, no animals were handled or subjected to experimental treatments of any sort over the course of data collection. 

# Author Contributions
TB: conception, experiment design, field data collection. HRG: conception, experiment and recording-system design.

# Acknowledgements
We would like to thank Antoniya Hubancheva, for her constant help and support in the collection of the *Ushichka* dataset (also for christening the dataset with its Bulgarian name) and the Tabachka field crew of 2018. The electronics (Markus Abels, Hannes Sagunsky, Reinhard Biller) and Feinmechanik (Erich Koch, Felix Hartl, Klaus Pichler) teams were of invaluable help and support all through the process of experimental setup design and troubleshooting. We would like to thank Pranav Khandelwal for extensive help setting up the thermal camera calibration workflow and Hedrick Tyson for providing helpful feedback and estimating camera parameters. Fieldwork-wise, we are grateful to Joanna Furmankiewicz for supporting initial recordings at the Jaskinia Niedźwiedzia and Diane Theriault for providing advice on thermal cameras. Special thanks and acknowledgement to the field assistance of Aditya Krishna and Neetash MR, without whom the data collection would not have been possible. T.B. was funded by a doctoral fellowship from the German Academic Exchange Service (DAAD) and the International Max Planck Research School for Organismal Biology. H.R.G. was funded by the Emmy Noether program of the German Research Foundation (DFG, GO 2091/2-1, GO 2091/2-2). We would like to thank the members of the Acoustic and Functional Ecology groups and the Max Planck Institute for Ornithology, Seewiesen for its support and infrastructure. 

# References