---
title: "The *Ushichka* dataset: a multi-sensor, multi-channel dataset to unravel active sensing in groups"

author: "Thejasvi Beleyur, Holger R. Goerlitz"

date: "Document updated: `r Sys.Date()`" 

output: 
  bookdown::word_document2:
  fig_caption: yes
  pdf_document:
    extra_dependencies: ["babel","float"]
  latex_engine: 
    lualatex
abstract: Individuals in groups show collective behaviour by influencing and responding to their neighbours' movements. The behavioural heuristics and sensory inputs behind visually based collective behaviours of fish schools, mammal migrations and bird flocks are well understood. These visual passive sensing collectives scale well with group size given sufficient ambient light. However, how active sensing collectives manage to show their group behaviours remains unanswered. Active sensing animals like echolocating bats emit loud ultrasonic calls and listen for faint returning echoes to sense their surroundings. When echolocating in groups, the loud calls of each bat may mutually mask and interfere with individual echo processing. Active sensing collectives are thus not expected to scale well with group size. However, bats often aggregate in large groups during cave emergences and in mating swarms. Active sensing collectives pose various challenges such as technical issues in animal tracking, analysing audio with overlapping calls and non-negligible reverberation. To fill these gaps, we present the *Ushichka* dataset.  *Ushichka* is a multi-sensor, multi-channel dataset consisting of audio and video data of wild bats flying freely and echolocating in their natural cave environment. A LiDAR scan of the cave complements the dataset to provide spatial context for the bats’ behaviour. The dataset is, to our knowledge, unique in simultaneously recording wild bat collective behaviour with multiple sensors, consisting of individual-level data on movement trajectory and vocalisation. *Uschichka* is, to our knowledge, unique in simultaneously capturing bat collective behaviour with multiple sensors. While promising novel insights, *Uschichka* also presents a series of technical challenges ripe for inter-disciplinary collaborations such as LiDAR-thermal camera alignment, multi-channel call correspondence matching and automatic microphone position estimation. The processed dataset allows us to reconstruct the auditory scene of each bat in the group, to investigate the collective behaviour of auditory-guided animals (and to compare it to visually-guided animals), to parametrise existing models of collective behaviour, and to investigate the effect of group size on sensing, movement and the underlying sensorimotor algorithms. 

bibliography: ushichka_references.bib

---

```{r echo=FALSE, message=F, warning=F}
library(knitr)
```

Things that are not yet fixed:

* Figure numbering (knitr+bookdown only allows sequential numbering for PDF documents - needs to be taken care of in final version

#  Introduction

Many animals across the tree of life move and live in groups, and show impressive coordinated behaviours [@sumpter2006principles]. Coordinated behaviours shown in these collectives are reproduced by simple heuristics in models with agents responding independently to the movement and positions of their neighbours [@reynolds1987a;@vicsek1995;@couzin2002a]. These coordinated behaviours have been well investigated in visually driven animal collectives such as birds, mammals and fish [@ballerini2008a;@strandburg2013visual;@pita2016collective]. Visually-guided collective behaviour is (as first approximation) independent of the presence and number of neighbouring animals as vision is a 'passive' sensory modality [@nelson2006a]. In contrast to passive sensing animals, are 'active' sensing animals. Active sensing animals (*sensu stricto* [@nelson2006a]), such as bats, dolphins and electric fish, emit probes of energy and monitor the modulation of the probes by their surroundings. Echolocating bats emit loud ultrasonic calls, and listen to the returning echoes to detect objects around them [@Griffin1958]. When multiple echolocating bats echolocate together, they can start to negatively affect each other's ability to detect their own echoes [@ulanovsky2008bat]. Every bat emits loud calls, and is attempting to listen to its own echo. However, along with its own returning echoes, each bat hears loud 'non-target' sounds (calls and echoes of neighbouring bats) which hamper its own echo detection. A bat that cannot detect its own echoes is metaphorically 'flying blind', and risks colliding into its neighbours , and of more concern, into hard structures around it. As the number of bats in a group increases, the number of non-target sounds increases rapidly in a non-linear fashion (~$(N_{bats}-1)^2$). Sensory simulations show that in large groups of bats, a bat may be detecting its neighbours only every few calls - and not every time it emits a call [@beleyur2019modeling]. 

Despite the expected deterioration of echolocation with increasing group sizes, echolocating bats in nature are social and seen in very large aggregations in the form of mating swarms, cave roosts and evening emergences [@Ortega2016;@Erkert1982;@gillam2010tbrasiliensis]. The dynamics of active sensing groups is understudied [but see @theriault2010a], and perhaps likely to differ from passive sensing groups in the total number of detectable neighbours. Given sufficient light, an animal in a visually driven collective will be able to detect all neighbours in its vicinity even at large group sizes. In contrast, the number of neighbours potentially detected by a bat in a group decreases with group size [@beleyur2019modeling], leading to a limited sensing ability. Even in visual collectives, recent studies [@jadhav2022randomness] suggest individuals may actually only be responding to just one neighbour at a time - despite being able to detect all neighbours around them.  Additional computational modelling of generic groups [@bode2011a] with 'limited interactions' (where one neighbour is detected at a given time, and detection is proportional to proximity) are also able to recreate collective behaviours. Echolocating bat groups provide a parametrisable system to study how collective behaviour works under a gradient of limited-interactions, and to extract generalised principles that may apply to multiple systems. 

Here we present the conceptual motivation and technical equipment behind the *Ushichka* dataset. The word *Ushichka*  (Ушичка, *OO-shi-ch-kaa*) in Bulgarian has a dimunitive connotation for something with multiple ears. The name is chosen to highlight the fact that *Ushichka* is a  multi-sensor, multi-channel dataset which aims to unobtrusively 'eavesdrop' (with technological eyes and ears) groups of multiple bats echolocating in the wild. 

The *Ushichka* dataset  is, to our knowledge, a unique dataset in that it comprises of multichannel audio and video datasets of bats echolocating in groups in their *natural environment* captured by a LiDAR scan of the space. The multichannel nature of the dataset allows 3D localisation and tracking with high spatio-temporal resolution. Previous experimental work on multi-bat echolocation in the lab and field to date has mostly been with up to 2 bats a time [@jones1994individual;@goetze2016a;@giuggioli2015a;@necknig2011between;@fawcett2015clutter;@fawcett2015echolocation], or  multichannel data of solely video or audio [@theriault2010a;@lin2016bats]. The multi-camera, multi-mic nature of *Ushichka* allows us to explore echolocation in groups in much greater detail than previously attempted. 

## The experimental study of echolocating bat groups

After the formal discovery of echolocation in the 1940's [@Griffin1958;@Dijkgraaf1946], the field of echolocation has moved leaps and bounds. A detailed understanding of the physiology, behaviour, neurobiology and acoustics behind echolocation has been uncovered in great detail [@popper1995hearing;@Fenton2016]. While a large body of knowledge is in place about how *individual bats* are able to echolocate through the use of trained animals in laboratory settings and field observations [@Fenton2016], comparitively little is known on how they echolocate in groups [@ulanovsky2008bat].

Before the digital revolution, the challenges in echolocation fieldwork included bulky analog recording and signal analysis equipment; limiting the speed and quantity of data that could be collected and analysed [@Griffin1958;@habersetzer1981a]. The onset of the digital revolution allowed for lightweight devices that can collect more data, which can then be analysed more easily. Current experimental approaches to study echolocation in groups include, for example, on-body tags that record audio from the focal animal and its neighbours (along with GPS and accelerometer data)[@egert2018resource;@cvikel2015b;@corcoran2021silence], microphone-camera pairs to quantify broad echolocation characteristics with group size [@lin2016bats] and thermal camera 3D tracking with sophisticated computer vision to track individuals in the million-strong emergence behaviour of *Tadarida brasiliensis* [@betke2007tracking;@theriault2010a]. Each of these studies have provided exciting and important insights into how echolocating bats manage to echolocate in groups. For instance, @cvikel2015b showed that *Rhinopoma microphyllum* do not seem to show the 'jamming avoidance response' (shifting call frequencies to avoid  overlap with neighbours, sensu @ulanovsky2004dynamics) first seen by @habersetzer1981a in its congener *R. hardwickei*. @lin2016bats find that *Miniopterus fuliginosus* bats echolocating in caves actually increase their pulse rate with group size, in contrast to theoretical expectations and results from *T. brasiliensis* in captivity which decrease their pulse rate [@jarvis2013groups]. @theriault2010a advanced the field by quantifying the geometry of a *T. brasiliensis* emergence, showing that animals in larger groups fly relatively close to each other (0.5\ m), and that neighbours are placed uniformly across azimuth and elevation around individuals, unlike the elevation-azimuth asymmetry seen in bird flocks [@ballerini2008a]. 

*Ushichka* represents a natural step forward to all the past work in the study of group echolocation in the field. The one important respect where *Ushichka* differs from previous work is its inherent 'multi-modality', in that it consists of simultaneously captured multi-microphone, multi-camera data streams, along with 3D scans of the recording volume for physical context. This multi-modality allows the simultaneous capture of the echolocation and flight behaviour of multiple bat individuals in relation to each other and their surroundings. The acoustic data will provide information about the bats’ sensory strategies to sample their world [@Fenton2016; @hein2020algorithmic]. Furthermore, using acoustic tracking, the audio data provides each bat’s spatial position each time it emits a call [@holderied2003echolocation; @Hugel2017; @surlykke2008echolocating;]. The video data is an independent way to calculate the bat’s 3D positions [@giuggioli2015delayed; @theriault2010a], including also for non-echolocating individuals. In complex audio recordings, position data from video-tracking can also be used for cross-validation with acoustic localisation data. Furthermore, it allows observing non-acoustic behaviour that is not visible in the acoustic data. Lastly, the LIDAR data provides the spatial context of the natural environment, in which the behaviour is happening.   In contrast, on-body tags and single microphone recordings lack explicit information of neighbour location and how many neighbours are there around the focal bat. Pure video tracking with one or more cameras does not provide insights into the bats' echolocation. By capturing both flight and echolocation data of multiple individuals in the group, *Ushichka* promises acce flight and echolocation data of the group, and holds promise to provide access to the auditory scene [@Moss2001] of each individual in the group. Reconstructing individuals’ sensory inputs together with their motor behaviour will offer detailed insights into animals’ sensory-motor algorithms and behavioural decisions [@strandburg2013visual]. Sensory reconstruction is an established method in visually driven animal groups like fish - while auditory scene reconstruction with movement analysis is an exciting new frontier for the field of active sensing collectives. Comparing between sensory systems will address system-specific mechanisms, as well as common functional and ecological principles behind how animals sense and respond to stimuli.

#  Experimental methods

## Field site and study species 
We recorded wild echolocating bats were made in the Orlova Chuka cave system (near Ruse, Bulgaria) in the first chamber  encountered after the entry corridor (Figures \@ref(fig:cavesetup), \@ref(fig:cavesetup2)). Data was collected across 14 recordings sessions (*w x l x h*~5x9x3m$^{3}$) between 19$^{th}$ June to 19$^{th}$ August 2018 (Supplementary Information (SI): Table 6.1). One recording session typically started around sunset and ended around sunrise the next day (see Sec. \@ref(recschedule)).

Orlova Chuka is known to host at least nine species of bats [@Ivanova2005;@govtbatcount]. During the summer, resident  *Myotis myotis* and *Myotis blythii* females together with their offspring form the dominant bat population. The *M. myotis* and *M. blythii* (hereon referred to as *M. myotis/blythii*) female bats arrive in spring and spend the summer in the cave system raising their young. Though rigorous mark-recapture data is missing , individual mothers consistently return to the cave system over multiple nights (pers comm, Laura Stidsholt, Stefan Greif). Given the numerical dominance of  *M. myotis/blythii* in the cave system, it can be fairly said that the *Ushichka* dataset consists mainly of these two species. Since the calls of *M. myotis* and *M. blythii* are indistinguishable (their morphology is also extremely similar too) [@dietz2016bats] we treat them as one group of bats. The other species in the cave stand out in their echolocation calls. Frequency modulating bats such as *M. daubentonii, M. capacinii, Miniopterus schreibersii* have very different call structure, while the constant frequency bats *Rhinolophus ferrumequinum, R.euryale are R. mehelyi* also easily identifiable [@dietz2016bats]. 

```{r echo=FALSE}
# Bat flight and echolocation were recorded in the same recording volume across a period of two/three months with more or less the same recording setup (3 thermal cameras, 12-22 microphones). During both morning and evening recording sessions, bats typically flew a few rounds in the recording volume (*w x l x h*~5x9x3m$^{3}$) before heading back into the direction they came from, or flying into another direction. Recordings were conducted 
```

```{r cavesetup, echo=FALSE, fig.cap='The *Ushichka* setup. The large inverted T shaped array in the centre of the image is the 120cm tristar microphone array with four SANKEN CO-100 microphones. The three blue lights (top-left, bottom-middle, top-right of image) originate from the three thermal cameras pointing towards the array. On the left-hand side of the cave wall, multiple smaller Knowles SPU 0410 microphones are attached to the wall on the left portion of the image (not visible, see  Figure \\@(fig:cavesetup2). Photo by Stefan Greif.'}
cavesetupimg = 'figures/ushichka_setup_DC6A5930_w.JPG'
include_graphics(cavesetupimg)
```

```{r cavesetup2, echo=FALSE, fig.cap=' Multiple Knowles SPU 0410 microphones (at the end of the black cables) were directly mounted on the left-hand side wall in Figure \\@ref(fig:cavesetup). Bats regularly flew by the microphones, seen here: *M. myotis/blythii* . Photo by Stefan Greif.'}
cavesetupimg2 = 'figures/DC6A6061.JPG'
include_graphics(cavesetupimg2)
```

## Audio, video and LiDAR systems 

### Audio  

Multichannel audio data was recorded by multiple (2-3) ASIO-protocol audio-interfaces (Fireface 802 and Fireface UC; RME GmbH, Haimhausen, Germany) running at 192kHz connected to a laptop (DELL Latitude E6540, Windows 7 Enterprise, Intel Core i5, 16GB RAM). The laptop was verified not to emit ultrasound. The number of channels of audio varied between 12-22 channels over the course of the field season. The microphone array consisted of four condenser microphones (CO-100, Sanken, Tokya, Japan) with the rest of the 8-18 channels being MEMS microphones (SPU-0410 Knowles, Itasca, USA). All microphones were connected to either the inbuilt pre-amplifiers on the audio-interfaces, or to two QuadMic II pre-amplifiers (RME GmbH, Haimhausen, Germany) . Later recordings with the *Ushichka* setup also included a Focusrite Scarlett OctoPre (Focusrite Plc, Bucks, UK) pre-amplifier. 

During all recording nights, the four Sanken CO-100 microphones were configured as a 120-cm tristart microphone-array, which was placed in a fairly constant position in the recording volume (front of a memorial plaque on the cave's wall; Figure \ref(fig:cavesetup)). A 'tristar' array is a common planar array configuration [@Hugel2017;@Goerlitz2010;@Lewanzik2018], with one central microphone surrounding by three microphones at a fixed radius and 120$^{\circ}$ angular separation. The Knowles SPU-0410 microphones were mostly mounted on one wall of the recording volume to increase the coverage of recorded echolocations. Microphone positions were constant within a recording session, and were often changed between sessions (except for a few occasions, where microphones were kept in the same position across consecutive nights). Microphones were not left in the cave across multiple days to avoid humidity and moisture droplets affecting the electronic circuits, microphone membranes and sound inlets. 

We used two approaches to measure microphone postions. First, a subset of all possible inter-mic distances were measured every recording session using a laser range finder (1mm accuracy, GLM 50C, Bosch, Gerlingen-Schillerhoehe, Germany). Second, ultrasonic sweeps were played back from multiple points in the recording volume as part of an  automatic mic position estimation workflow. Acoustic calibration of the frequency response and directionality was performed at least once for most of the microphones after the field season. For more information on the microphone automatic position estimation and calibrations please refer to the Section \@ref(noframes) and the SI. 

### Video  

Three thermal cameras (9mm focal length, TeAx ThermalCapture 2.0, TeAx Technology GmbH, Wilnsdorf, Germany) running with FLIR Tau 640 cores (640x512 pixel image resolution) at 25Hz frame-rate formed the camera array. All three cameras (Figure \@ref(fig:multicambats)) were frame synchronised.

```{r multicambats, echo=FALSE, out.width="100%",fig.cap="Thermal video recording. Top: Synchronised frames of the three thermal cameras, recording multiple bats flying the recording volume for subsequent 3D tracking. Bottom: A view of the cameras in the recording volume."}
multicambatsimg = 'figures/multicam_views.png'
include_graphics(multicambatsimg)
```

The thermal cameras recorded video directly onto microSD cards (SAMSUNG PRO+, 32 GB), which were inserted into each camera at the start of each recording session. Since removing the microSD card from the cameras significantly disturbed their original position, the total video recording time was limited by the microSD card memory capacity, and video data was thus downloaded after the end of the recording session. The position and orientation of all cameras were calibrated in the field using the 'wand' protocol of @Theriault2014. For further details on camera intrinsic and extrinsic parameter calibrations, wand setup, and synchronisation with audio - see SI 4. 

### LiDAR 
We performed a LiDAR-scan of the cave surface, along with further large parts of the Orlova CHuka cave (Fig. 2.4). This dataset provides high-resolution (<1 cm) 3D-data of the cave interior as the spatial context for the bats’ observed echolocation and flight behaviour of the bats [@bggeospatial]. Without this environmental data, we will at most be able to recreate each bat’s positions and call timing relative to the other bats. Only after aligning the coordinate systems of the acoustic and video bat data with the cave’s LiDAR data, we will have access to the bats’ absolute position in the cave. Since bats react both to each other’s presence as well as to their physical environment, only such a combined dataset allows a complete understanding of the bats’ sensory-behavioural algorithms, their inter-individual interactions and collective behaviour, and their reaction to their environment. 

```{r lidarimage, echo=FALSE, fig.cap='The LiDAR map of the recording volume showing the same view as in Figure \\@ref(fig:multicambats), with the large tristar array.'}
lidar_scan_img = 'figures/lidar_image.png'
include_graphics(lidar_scan_img)
```

### Weather data
The local weather conditions (relative humidity, temperature and atmospheric pressure) within the cave were recorded continuously by a weather logger (Kestrel 4000 , Nielsen-Kellerman Co., Boothwyn, USA), typically hung from the tristar array stand. Weather conditions determine the speed of sound and the atmospheric attenuation of sound  [@goerlitz2018weather;@lawrence1982measurements], and thus influence the accuracy of acoustically tracked bat positions [@goerlitz2018weather] and source level estimates. 


## Recording schedule {#recschedule}
The audio and video arrays were mostly setup before sunset (~21:00 Eastern European Time) and collected data until around sunrise (~5:30 EET), with one break in between around 12-3 am, which coincided with an observed drop in bat activity. In two of the 14 recording sessions, recordings were triggered manually when bat calls were heard on a bat detector (SI Table 1). The recordings of the remaining sessions were triggered automatically when bat calls on a subset of the monitored channels surpassed a threshold between -50 to -40 dB RMS FS, which was adjusted across between sessions over the field season. Recording were stopped after a pre-defined duration of 10-15 s (variation due to adjustments made over the field season). There were no other ultrasonic sources in the cave, and we are certain that all recordings were triggered by bat calls only. Automatic recording triggering implemented in the ```fieldrecorder_trigger``` module relies on the open-source Python language [@van2011python], and the sounddevice, scipy and numpy [@geier2015a;@virtanen2019a;@oliphant2006a] packages. 

The almost constant bat activity in the cave would continuously trigger our recording system. This would cause unmanageably large data files and fill-up the microSD cards (32 GB) of the TeAX thermal camera system before the end of the night. Thus, we set a duty-cycle for the audio-video recording system (of around 20%), which determined, together with the recording duration, the pause duration between subsequent recordings. For example, a duty cycle of 20% and a recording duration of 10 s meant that recordings could be triggered at most every 50 seconds (10 s of recording followed by 40 s of recording pause). On a few occasions, morning and evening recordings were missed due to various technical and logistical issues.

```{r multichannel-calls, echo=FALSE, fig.cap="Example audio data of bat calls from a subset (four channels from top to bottom) of the multichannel recordings. The spectrograms highlight some of the challenges of working with the multi-bat audio data recorded in a cave, including reverberation, multi-path propagation, call overlaps, and correct call identity matching across channels. A) One bat; note the multiple echoes and reverberation following each call. B) Multiple bats; note additionally the overlapping calls and difficulty of determining identifying the call sequence of a single bat."}
multichcalls = 'figures/single_multibat_specgram.png'# 'figures/multichannel_composite_elongated.png' #'figures/multichannel_composite.png'
include_graphics(multichcalls)
```

## Bat activity
Bats arrived in the recording volume from their further inward-lying roosting sites around sunset. The first activity typically consisted of small groups (N=2-5) of *R. euryale* and *R. mehelyi* flying rapidly and unpredictably in the volume. Subsequently, the number of *M. myotis/blythii* bats slowly increased, reaching up to 30 bats flying simultaneously in the recording volume. The *M. myotis/blythii* bats also started to form temporary roosting clusters on the walls of the recording volume towards end of June, yet with decreasing frequency as the season progressed further. These temporary roosts lasted for a few hours from sundown until midnight, after which they dispersed. During cloudy and rainy evenings, the emergence from the cave was delayed or ceased completely. 

Peak bat activity in the recording volume was typically seen about 1-1.5 hours after sun-set, after which bats began to exit the cave (Figure \@ref(fig:activity)). The exit happened relatively rapid, after which only a few bats were seen to still fly in the recording volume. Individual bats returned in the early morning from one to two hours before sunset onwards. It seemed that *R. euryale* and *R. mehelyi* activity remained fairly constant through the night, even during the drop in general activity around midnight. 

```{r activity, echo=FALSE, fig.cap='Temporal pattern of bat activity. Schematic of typical bat activity pattern of Myotis myotis and Myotis blythii between sunset to sunrise in the *Ushichka* dataset. The multichannel recording system was typically activated before sunset and was run till after sunrise with a break in between'}
activityimg= 'figures/ushichka_activity.png'
include_graphics(activityimg)
```

# Technical challenges ahead and potential solutions in analysing the *Ushichka* dataset

To realize the true potential of the Ushicka dataset for studying echolocation and flight in groups, we will need to surmount a series of challenges related to analysing multi-call echolocation data. Here they are discussed in increasing order of difficulty.

1) **Sound reflections** : The cave walls surrounding the recording volume are strong acoustic reflectors, leading to significant multi-path propagation. Multi-path propagation occurs when a sound arrives multiple times at a microphone due to multiple distinct reflections at different surfaces. For instance, a specific call arrives first at the microphone via its direct path from the bat to the microphone. It is then followed by the multiple reflections from the ground, the cave walls and the ceiling at different delays depending on the total travelled distance. With increasing number of potential reflectors and bats, multi-path propagation will become increasingly difficult, since N reflectors and M emitted calls could cause up to NxM recorded call reflections. Acoustic tracking of bat position depends on calculating the time-difference-of-arrival different (TDOA) of the same call at the different microphones, typically via cross-correlation of the audio channels. The many false ‘copies’ of the original call within each channel can create spurious peaks in the cross-correlation, causing wrong TDOA estimates and thus location errors. Solutions to handling multi-path propagation exist, such as the DATEMM algorithm and its variants  [@kreissig2013fast;@scheuing2008disambiguation;@zannini2010improved].For the example the zero-sum  TDOA criterion for one sound recorded on three channels is used to identify reliable TDOA measurements in @kreissig2013fast - followed by a 'graph-synthesis' approach to genereate a full set of TDOAs for source localisation.

2) **Call density** : Each bat flying in and out of the volume emits about 10-20 calls per second (10-20 Hz call rate). The acoustic localization of one bat is relatively straightforward since calls are separated by tens of milliseconds. With increasing number of bats, the temporal separation between individual calls decreases and the probability of calls overlapping in time increases. Analysing calls at such high densities is challenging and will require the latest techniques for call detection in the presence of overlaps  [@izadi2020separation], or in general blind signal separation techniques [@brandstein2013microphone]. 

3) **Correspondence matching** : In addition to reliably detecting individuals calls in single channels, the same calls must be identified across channels, a common problem in any multi-channel acoustic array [@brandstein2013microphone].Specifically, an echolocation call detected in one channel needs to be matched to itself in another channel, despite the possible presence of multiple similar calls in the expected time-window. Individual bats may have unique call characteristics (@masters1995sonar, and in *M. myotis*: @yovel2009voice; but also see @siemersnovoice).However, since manual correspondence matching is prohibitive for larger group sizes and numbers of calls to be analysed, robust automated algorithms are needed to solve this problem. Furthermore, echolocation calls have very flexible acoustic characteristics, which bats constantly adapt to the current task [@Goerlitz2010;@lewanzik2021task;@Fenton2016], and which may override potential individual-specific acoustic signatures, rendering bat echolocation calls difficult for call classification. DATEMM-type algorithms [@kreissig2013fast;@scheuing2008disambiguation] may present a solution. Much like multi-path propagation, the presence of overlapping sounds also creates spurious peaks in cross-correlations between two channels. For example @kreissig2013fast is able to handle and localise overlapping sounds by filtering cross-correlation peaks and TDOA combinations based on the assumptions that follow from sources that have direct path propagation. While the output of a DATEMM type localisation is the 3D positions of the sources, these sources correspond to a set of TDOAs across channels. Knowing the candidate TDOAs may allow post-hoc matching of the emitted calls across channels. 

Based on the multi-channel acoustic data recorded by multiple microphones at known positions, the position of vocalising animals can be calculated for each received call (“acoustic tracking”, @aubauer;@aubauerRuppert). The basic principle of acoustic tracking relies on the fact that sound travels at a limited speed and thus arrives at different times at different positions in in space. When comparing the sound arrival time between at least four microphones, each position of sound emission will generate a unique set of TDOAs between those microphones. Measuring TOADs thus allows to calculate the position at which a bat emitted a call. With increasing number of bats and reflections, acoustic tracking will become increasingly challenging, as discussed above. Our multi-camera video data will provide one important ‘support system’ to solve the issues of multi-path propagation and correspondence matching. The time-synchronised video tracking data provides an independent estimate of the 3D-positions of the microphones and bats. By combining the audio and video data, we can simplify cross-channel correspondence matching and the process of assigning a call to its source bat. Each call is emitted while the bat is at one position in space. The bat’s position is recorded on video, while the call is recorded on multiple audio channels, resulting in one correct set of TOADs for this spatial position. The video-observed bat positions represent all possible points of call emission over time. Based on the bat-microphone distances, we can generate a set of ‘predicted’ TOADs between microphone pairs over time. The problem thus boils down to a matching problem between measured and predicted TOADs. Given a set of video-based bat flight trajectories, we are able to filter out a larger number of spurious TOADs. The trajectory based filtering approach could in principle elegantly solve the problems of multi-path propagation and cross-channel correspondence at one shot. It still remains to be tested, however, whether the video-based 3D-positions are of sufficient accuracy to provide sufficiently accurate TOAD predictions.  


```{r echo=FALSE}
# Implemented all HRG suggested changes till here 30/11/2022 -16:55
```
## Fusing multiple data streams to reconstruct individual auditory scenes
Reconstructing individual auditory scenes requires recreating the incoming sounds (echoes, neighbour's calls) and outgoing sounds (emitted calls) experienced by each bat in a group. Emitted sounds can be assigned through acoustic tracking of calls using the microphone array. The incoming echoes a bat hears in the recording volume will come from targets such as other bats and the walls of the cave. The incoming calls will be from the calls emitted by neighbouring bats. Recreating the incoming sounds heard by a bat thus requires knowing the relative positions of the bats in the group and their locations in the recording volume. Reconstructing auditory scenes in the group thus requires a fusion of the various data streams in *Ushichka*, along with their alignment into a common coordinate system.

The audio and video streams are tightly linked as they describe the flight and echolocation behaviour of the bats in the recording volume. However, the bats in the recording volume are likely trying to mainly avoid collisions with the walls and structures in the cave while the location of other bats is probably of lesser concern. This is where the LiDAR dataset informs the how and why of individual behaviours in the cave. Fusing the acoustic and video data streams is relatively straightforward in that they are time-synchronised and they both track common 'objects' (bats). The 3D positions derived from both data streams can be overlaid to match with a simple rigid rotation of coordinate systems. To understand the bats' echolocation and flight in their physical context however, the LiDAR scan of the recording volume must be aligned with the coordinate systems of the camera array. This will allow us to interpret the sensory decisions of bats correctly, eg. whether a sudden turn was due to proximity to the cave wall. 

Recent advances in the alignment of LiDAR scans with camera views have led to the formulation of 'targetless' workflows that do not require specific calibration objects for alignment [@kang2020automatic;@munozbanon2020]. These approaches rely on recreating synthetic camera images by projecting the 3D LiDAR scan into 2D (eg. Figure \@ref(fig:multicambats)). The best LiDAR-camera alignment is one which which achieves the highest similarity between the synthetic 2D projection and observed camera image (or some transformation of the images). *Ushichka* is fit to use the alignment methods mentioned above as the extrinsic (position and rotation) and intrinsic (eg. focal length, distortion) camera parameters required for the 2D projection are known for each recording session. One potential issue that may arise is that the methods of  @kang2020automatic and @munozbanon2020 have been developed for visible light cameras. Thermal camera images often look very different from light camera images, and may require different processing steps. To our limited knowledge current thermal camera-LiDAR alignment protocols require a specifically designed calibration object [@krishnan2017cross;@slatcalib;@zhangcalib], and  there is limited work on target-less alignment of LiDAR and thermal data [@phuc2017registration]. While it may be possible to use the target-less workflows for *Ushichka*, the lack of thermal dynamic range in images may present an additional challenge. In contrast to RGB images, thermal images are mono-channel and the cameras we used also have a limited thermal resolution (~1$^{\circ}$ C). In addition to device limitations, the actual thermal environment in the cave may pose a challenge. The cave system maintains a relatively stable temperature of around 10$^{\circ}$ C throughout the year. From a thermal perspective, the constrast between one part of the cave and another may not be as large as what is seen in many RGB images of a given view. Despite these challenges, one advantage *Uschichka* may have is that each recording session has *three* camera views (instead of one) that can be used to align the LiDAR scan. In this respect, our three camera situation may result in a more constrained alignment despite the poor thermal gradient and device limitations in the thermal images. 

## Automatic mic position estimation removes the need for bulky frames {#noframes}

Working with multi-microphone arrays typically includes carrying many long cables and heavy frames to hold microphones in known positions. Microphone positions need to be known for successful acoustic localisation, and precise mic positions lead to lower localisation errors [@Wahlberg1999].  However, carrying large array frames into the field, especially in cluttered environments such as caves or forests is often difficult and time-consuming. In response to these novel conspicuous objects, bats may actually focus their calls specifically on the frames and the recorded behaviour might actually be an unwanted artifact of the setup itself. There is thus a pressing need in bioacoustics for the development of acoustic arrays that are less conspicuous, and can be setup universally and quickly. One option, as seen in *Ushichka*, is to place the microphones on existing structures such as cave walls and rocks (branches, signposts and other structures may also be useful in other field sites). Using existing structures to hold microphones does not generate new obstacles or points of interest, and is thus less likely to generate artifactual inspection behaviours from the animal. 

Once frames have been abandoned in favour of existing structures, the task of measuring microphone positions still remains. The most accurate solution to directly measure microphone positions  is to use a Total station theodolite. Total station's have measurement acccuracies of a few mm, and directly output the 3D positions of microphones. A major issue with the use of a Total station is the addition of yet another piece of equipment, and the time taken to set up the device itself for measurements. Alternately, it is possible to estimate mic positions indirectly by exhaustively measuring inter-mic distances using a laser range finder or tape-measure (as has been done for *Ushichka*). However, experience has shown manual inter-mic measurements can be error-prone at larger distances (~4-5m) with a laser range finder due to jittery hands, and become impractical with tape-measures. Manual inter-mic distance measurements require direct line-of-sight between microphones and do not scale well with an increasing number of microphones in an array (number of measurements to be made scales rapidly with $\frac{N_{mics}\times N_{mics}-1}{2}$).  

Given the problems around microphone position measurements without frames, we were looking for solutions that have the ease of today's camera calibration workflows [eg. @Theriault2014], and found the 'Structure-from-Sound' approach of @zhayida2016automatic. @zhayida2016automatic were able to infer microphone positions based only on commonly recorded playbacks, and without the need for any actual inter-mic distance or position measurements. We were also able to verify the utility of the Structure-from-Sound approach for our freely-placed microphones on ground-truthed data [@sfs_cotdoa, also see SI]. Such automatic position estimation workflows have the potential to drastically reduce the effort needed to run multi-channel recordings in the field, and open up the exciting realm of inconspicuous experimental setups. We are currently continuing the collaboration and looking to formulate mic position estimation workflows that are field-friendly and widely applicable across diverse field-sites.

# Investigative opportunities opened by the *Ushichka* dataset

Here we detail some of the possible lines of investigations that can be undertaken with *Ushichka* once the necessary analytical workflows are in place. 

## Collective behaviour in active sensing groups 
The combination of acoustic and video tracking along with a LiDAR scan of the cave provides us sufficient data to reconstruct the sensory inputs of each bat in a group. To reconstruct the sensory inputs of each bat, we would require baseline data on a series of parameters such as call and hearing directionality, auditory masking, hearing threshold and source level. These parameters can either be extracted from the literature (as done by @beleyur2019modeling and @mazar2020sensorimotor) or measured using the acoustic tracking system itself (eg. source level and call directionality). Given a group of *N* bats, we can thus reconstruct all the sounds each bat would have heard in their inter-pulse intervals (the silent gap between calls). Previous studies have attempted to simulate such contexts with biologically parametrised values of the input sounds [@beleyur2019modeling;@mazar2020sensorimotor]. Our experimental measurements in combination with detailed sound propagation models could allow for *direct* sensory input reconstructions in a whole group of bats simultaneously. Knowing the sensory inputs animals receive over time would allow us to finely parametrise the relevant models of collective behaviour and test how well they predict group  motion. For instance, @bode2011a model a group of agents that perform non-simultaneous and discontinuous 'updates' of their environment. Their model is very similar to how groups of bats get information about their surroundings, as each bat is very likely emitting calls independently [@hase2018a]. @bode2011a moreover model 'limited-interactions' in collective motion, where each agent can only detect the positions of one neighbour  at each update, very similar to what bats may be experiencing in large groups [@beleyur2019modeling]. Using the 'limited-interactions' model, @bode2011a are able to show that their predictions match empirical observations of starling flock structure. More importantly, @bode2011a also observe that their 'limited-interactions' model results in quantitatively different group structure in comparison to commonly used zone-based models [eg. @couzin2002a]of collective behaviour. The @bode2011a model can be parametrised by estimating how often bats call in a group (update rate), and how many neighbours they are typically able to detect (eg. using the framework in @beleyur2019modeling). The motion of *in silico* active sensing groups can then be compared with observed bat groups. Sensory reconstruction approaches in echolocation would also open the field to fine-scale temporal analyses of echolocation and flight behaviour: do bats turn away more often in response to an echo from a wall, or from the direction of a calling neighbour? Do bats call more often when they may have 'missed' echo detections due to call overlaps in the past few interpulse intervals?

## Optimising echolocation and flight strategies under challenging conditions
Bats are known to alter their echolocation rapidly according to the behavioural context, and ambient soundscape [@corcoran2017sensing]. *Ushichka* offers direct access to bat echolocation strategies as they are  acoustically challenged by increasing group sizes (Fig \@ref(fig:activity)). Aside from call-level data such as source-level, duration and spectral properties - the presence of multiple microphones spread across the room also provides access to call direction and beam shape modulations (Figure \@ref(fig:beamshape)A). While other call parameters have been investigated in studies with multiple animals, call directionality and beam-shape measurements have so far mainly been done with single animals in the field and lab [@surlykke2012a;@giuggioli2015a]. Revealing where bats aim their calls, and how they spread the energy in each call will uncover the sensory priorities bats may have while flying under challenging conditions. Do bats choose to focus narrowly on fast moving conspecifics, but broadly onto bigger obstructions like walls? Does their call direction and beam shape change with increasing group size? 

```{r beamshape, echo=FALSE, fig.cap="The benefits of multi-mic arrays for the study of echolocation and voice recognition (microphone: blue circle with line attached) A) Biosonar beam-shape can be estimated from microphones spread around the animals B,C) As the bat moves through the volume (C 1-8), the amplitude and spectral properties of the call received by the focal microphone (red) changes, as seen on the spectrogram (B). Having multiple microphones provides a multi-view picture of a call from many directions, allowing a better assessment of intra-individual call variation required for robust voice recognition"}
beamshapeimg = 'figures/beamshape_cleaned.png'
include_graphics(beamshapeimg)
```

The echolocation strategies in use may also inform why certain types of apparently stereotypical flight paths are seen within the recording volume. It was observed that bats sometimes tend to fly a circular loop close to the edges of the recording volume before leaving the cave or heading back into the roosting site (Figure \@ref(fig:loopingsch)). This 'looping' flight behaviour was observed in the evening flight activity as bats began to accumulate in the recording volume, but also when bats returned alone in the morning. Bats are known to show  stereotypic flight behaviours in the lab and in the wild [@mohres1949versuche;@barchi2013spatial], and this may perhaps be due to their limited attention or spatial memory.  However, in a cave setting, where there are no obvious obstacles, what drives the formation of these looping behaviours? Are the loops a stereotypic trajectory that allows the bat to limit its attention only along that route? Why do bats not take the shortest path and fly from the exit straight into roosting site? Initial observations of group flights suggested that even when multiple bats were flying together, not all parts of the recording volume were being 'used' by the bats. Some regions of the recording volume (regions close to walls)  seemed to have a higher density of bats. Is this wall-clinging behaviour a behavioural adaptation to maintain a high received level of wall-echoes (in the presence of non-target sounds)? The wall-clinging behaviour is somewhat reminiscent of structure-following behaviour seen in the field with horsehoe bats, that follow hedgerows instead of taking shortcuts over open fields. Horseshoe bats however, are likely to follow structures due to the limited sensing range from their high-frequency echolocation - why would myotids also adopt this behaviour? Another interesting parameter to estimate is the dynamics of the loop's chirality (clockwise/counter-clockwise) with group size. Alignment is a standard measure of how similar animals are in their direction of movement. When there are multiple animals in the recording volume, a measure of the alignment over time and group size might reveal whether the looping behaviour is an individually driven behaviour or is modulated by the flight directions of group members. 


```{r loopingsch, echo=FALSE, fig.cap="Schematic showing a view of the direct and looped flight paths as seen from above. The direct (broken blue line) and indirect (continuous green line) paths are flown in both directions (exit/entrance-cave interior, cave interior-exit/entrance). The grey quadrilateral outlines the recording volume."}
loopingimg = 'figures/looping.png'
include_graphics(loopingimg)
```

## Voice recognition for short-term and long-term behavioural tracking 
Many vocalising vertebrates across the animal kingdom have individually identifiable vocalisations, or 'voices' [@carlson2020individual]. The voice of each animal is a temporally stable cue, unlike experimental markings which may be lost over time. Despite the known stability of voice, for some animals one of the main challenges with developing a stable voice identifier is the variation in the vocalisation itself. Bird call repertoires can change over time, and complicate identification [@stowell2019automatic]. Approaches to date rely on machine-learning and statistical models, which require a training dataset of example vocalisations to 'learn' the patterns in the data. Variation caused by sound directionality, distance and ambient noise may often necessitate larger training datasets, which are often hard to obtain [@stowell2019automatic]. Unlike many types of animal vocalisations (eg. bird song, mammal social vocalisations), the spectro-temporal structure of individual bat echolocation calls are stereotypical and likely to remain stable because they serve a single sensory purpose – to detect objects. The dedicated sensory functionality of echolocation calls, and the general absence of background ultrasonic noise in the wild makes bat calls a convenient system to develop voice identification workflows.

Bat echolocation calls can be assigned to individuals by their acoustic features, though this has typically been done in small lab populations. Previous work [@masters1995sonar;@yovel2009voice] solved the 'closed-set' problem where all animal identities are known. *Ushichka* provides a large dataset over multiple nights and animals that fly by [also see Figure \@ref(fig:beamshape)B,C]. In contrast to the 'closed-set' problems solved in the past, *Ushichka* could be used to generate methods to solve the 'open-set' voice recognition task. Open-set classification problems deal with models that uniquely identify individuals that they have been trained on, but also those they have *not* been explicitly trained on. Even though the identities of bats echolocating in the recordings are unknown, it may be possible to exploit the behavioural patterns of the animals to approximate identity. Bats always return alone in the morning, and these recordings contain multiple echolocation calls from one individual as it enters the recording volume and heads away to the roosting site. In addition, it is known that a stable population of resident female *M. myotis/blythii* roost in the cave system over the entire summer period. Using these two facts, if successful, a voice recognition model should at least show certain patterns:  1) correctly assign identity to an independent subset of calls it was not trained on (validation data) from morning recordings 2) identify at least some individuals consistently across multiple morning returns, and in the best case 3) be able to pick up the individuals as they begin their activity in the recording volume around sunset. Of course, even if a trained model satisfies the three criteria above, it does not directly follow that the detections in scenarios 2) and 3) are not false positives. It may be necessary to systematically collect a larger sample of echolocation calls from hand-released individuals. The state of signal-processing and machine learning know-how in the field of human voice recognition is relatively mature at this point, with the use of speaker-specific Gaussian Mixture Models compared to Universal Background Models trained against multiple individuals [@Hennebert2009]. *Ushichka* may provide a test-dataset to examine how well the latest voice recognition approaches perform for echolocation calls. 

While seemingly challenging, if a validated voice recognition model is indeed developed successfully, it will allow revolutionary markerless short-term and long-term tracking of individual behaviours. Short-term behaviours such as flight and echolocation decisions over the course of a few seconds can provide insights into  individual sensorimotor strategies and the tracking of social interactions. Long-term decisions such as the change of evening cave exit-time and morning return times can reveal the changing physiological status of the resident females as their pups mature. Voice based individual recognition brings the same advantages gained by  tracking mammals  with unique visible markings (eg. zebras, tigers) [@lahiri11_biometric;@hiby2009tiger], allowing unobtrusive observations over multiple time-points and scales.

# Supplementary code and website

The list of supplementary code, data and information are below:

#. The ```fieldrecorder_trigger``` module used to trigger automatic recording is available at the following repository:[https://github.com/thejasvibr/fieldrecorder](https://github.com/thejasvibr/fieldrecorder).

#. The speaker playback data to benchmark automatic microphone self-positioning is available at: [https://doi.org/10.5281/zenodo.5126695](https://doi.org/10.5281/zenodo.5126695)

#.More snippets of the the *Ushichka* dataset and research progress is documented at:  [https://thejasvibr.github.io/ushichka/](https://thejasvibr.github.io/ushichka/).

# Field work permits 
All field work at the Orlova Chuka cave was performed under  license of the relevant local authorities (Permit # 795/17.05.2019). Ushichka is a purely observational dataset, no animals were handled or subjected to experimental treatments of any sort during data collection. 

# Author Contributions
TB: conception, experiment design, field data collection, lead writing. HRG: conception, experiment and recording-system design, writing.

# Acknowledgements
We to thank Antoniya Hubancheva, for her constant help and support in the collection of the *Ushichka* dataset (also for christening the dataset with its Bulgarian name) and the Tabachka field crew of 2018. The electronics (Markus Abels, Hannes Sagunsky, Reinhard Biller) and Feinmechanik (Erich Koch, Felix Hartl, Klaus Pichler) teams were of invaluable help and support throughout the design and troubleshooting process of the experimental setup. We furthermore thank Pranav Khandelwal for extensive help setting up the thermal camera calibration workflow and Hedrick Tyson for providing helpful feedback and estimating camera parameters. Fieldwork-wise, we are grateful to Joanna Furmankiewicz for supporting initial recordings at the Jaskinia Niedźwiedzia and Diane Theriault for providing advice on thermal cameras. Special thanks and acknowledgement to the field assistance of Aditya Krishna and Neetash Mysuru, without whom the data collection would not have been possible. T.B. was funded by a GSSP doctoral fellowship from the German Academic Exchange Service (DAAD) and the International Max Planck Research School for Organismal Biology. H.R.G. was funded by the Emmy Noether program of the German Research Foundation (grant no. 241711556DFG). We  thank the members of the Acoustic and Functional Ecology groups and the Max Planck Institute for Ornithology, Seewiesen for its support and infrastructure. 

# References