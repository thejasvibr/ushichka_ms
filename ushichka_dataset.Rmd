---
title: "The *Ushichka* dataset: a multi-sensor, multi-channel dataset to unravel active sensing in groups"

author: "Thejasvi Beleyur, Holger R. Goerlitz"

date: "Document updated: `r Sys.Date()`" 

output: 
  bookdown::word_document2:
  fig_caption: yes
  pdf_document:
    extra_dependencies: ["babel","float"]
  latex_engine: 
    lualatex
abstract: Individuals in groups show collective behaviour by influencing and responding to their neighbours' movements. The behavioural heuristics and sensory inputs behind visually based collective behaviours of fish schools, mammal migrations and bird flocks are well understood. These visual passive sensing collectives scale well with group size given sufficient ambient light. However, how active sensing collectives manage to show their group behaviours remains unanswered. Active sensing animals like echolocating bats emit loud ultrasonic calls and listen for faint returning echoes to sense their surroundings. When echolocating in groups, the loud calls of each bat may mutually mask and interfere with individual echo processing. Active sensing collectives are thus not expected to scale well with group size. However, bats often aggregate in large groups during cave emergences and in mating swarms. Active sensing collectives pose various challenges such as technical issues in animal tracking, analysing audio with overlapping calls and non-negligible reverberation. To fill these gaps, we present the *Ushichka* dataset.  *Ushichka* is a multi-sensor, multi-channel dataset consisting of audio and video data of wild bats flying freely and echolocating in their natural cave environment. A LiDAR scan of the cave complements the dataset to provide spatial context for the bats’ behaviour. The dataset is, to our knowledge, unique in simultaneously recording wild bat collective behaviour with multiple sensors, consisting of individual-level data on movement trajectory and vocalisation. *Uschichka* is, to our knowledge, unique in simultaneously capturing bat collective behaviour with multiple sensors. While promising novel insights, *Uschichka* also presents a series of technical challenges ripe for inter-disciplinary collaborations such as LiDAR-thermal camera alignment, multi-channel call correspondence matching and automatic microphone position estimation. The processed dataset allows us to reconstruct the auditory scene of each bat in the group, to investigate the collective behaviour of auditory-guided animals (and to compare it to visually-guided animals), to parametrise existing models of collective behaviour, and to investigate the effect of group size on sensing, movement and the underlying sensorimotor algorithms. 

bibliography: ushichka_references.bib

---

```{r echo=FALSE, message=F, warning=F}
library(knitr)
```

Things that are not yet fixed:

* Figure numbering (knitr+bookdown only allows sequential numbering for PDF documents - needs to be taken care of in final version

#  Introduction

Many animals across the tree of life move and live in groups, and show impressive coordinated behaviours [@sumpter2006principles]. Coordinated behaviours shown in these collectives are reproduced by simple heuristics in models with agents responding independently to the movement and positions of their neighbours [@reynolds1987a;@vicsek1995;@couzin2002a]. These coordinated behaviours have been well investigated in visually driven animal collectives such as birds, mammals and fish [@ballerini2008a;@strandburg2013visual;@pita2016collective]. Visually-guided collective behaviour is (as first approximation) independent of the presence and number of neighbouring animals as vision is a 'passive' sensory modality [@nelson2006a]. In contrast to passive sensing animals, are 'active' sensing animals. Active sensing animals (*sensu stricto* [@nelson2006a]), such as bats, dolphins and electric fish, emit probes of energy and monitor the modulation of the probes by their surroundings. Echolocating bats emit loud ultrasonic calls, and listen to the returning echoes to detect objects around them [@Griffin1958]. When multiple echolocating bats echolocate together, they can start to negatively affect each other's ability to detect their own echoes [@ulanovsky2008bat]. Every bat emits loud calls, and is attempting to listen to its own echo. However, along with its own returning echoes, each bat hears loud 'non-target' sounds (calls and echoes of neighbouring bats) which hamper its own echo detection. A bat that cannot detect its own echoes is metaphorically 'flying blind', and risks colliding into its neighbours , and of more concern, into hard structures around it. As the number of bats in a group increases, the number of non-target sounds increases rapidly in a non-linear fashion (~$(N_{bats}-1)^2$). Sensory simulations show that in large groups of bats, a bat may be detecting its neighbours only every few calls - and not every time it emits a call [@beleyur2019modeling]. 

Despite the expected deterioration of echolocation with increasing group sizes, echolocating bats in nature are social and seen in very large aggregations in the form of mating swarms, cave roosts and evening emergences [@Ortega2016;@Erkert1982;@gillam2010tbrasiliensis]. The dynamics of active sensing groups is understudied [but see @theriault2010a], and perhaps likely to differ from passive sensing groups in the total number of detectable neighbours. Given sufficient light, an animal in a visually driven collective will be able to detect all neighbours in its vicinity even at large group sizes. In contrast, the number of neighbours potentially detected by a bat in a group decreases with group size [@beleyur2019modeling], leading to a limited sensing ability. Even in visual collectives, recent studies [@jadhav2022randomness] suggest individuals may actually only be responding to just one neighbour at a time - despite being able to detect all neighbours around them.  Additional computational modelling of generic groups [@bode2011a] with 'limited interactions' (where one neighbour is detected at a given time, and detection is proportional to proximity) are also able to recreate collective behaviours. Echolocating bat groups provide a parametrisable system to study how collective behaviour works under a gradient of limited-interactions, and to extract generalised principles that may apply to multiple systems. 

Here we present the conceptual motivation and technical equipment behind the *Ushichka* dataset. The word *Ushichka*  (Ушичка, *OO-shi-ch-kaa*) in Bulgarian has a dimunitive connotation for something with multiple ears. The name is chosen to highlight the fact that *Ushichka* is a  multi-sensor, multi-channel dataset which aims to unobtrusively 'eavesdrop' (with technological eyes and ears) groups of multiple bats echolocating in the wild. 

The *Ushichka* dataset  is, to our knowledge, a unique dataset in that it comprises of multichannel audio and video datasets of bats echolocating in groups in their *natural environment* captured by a LiDAR scan of the space. The multichannel nature of the dataset allows 3D localisation and tracking with high spatio-temporal resolution. Previous experimental work on multi-bat echolocation in the lab and field to date has mostly been with up to 2 bats a time [@jones1994individual;@goetze2016a;@giuggioli2015a;@necknig2011between;@fawcett2015clutter;@fawcett2015echolocation], or  multichannel data of solely video or audio [@theriault2010a;@lin2016bats]. The multi-camera, multi-mic nature of *Ushichka* allows us to explore echolocation in groups in much greater detail than previously attempted. 

## The experimental study of echolocating bat groups

After the formal discovery of echolocation in the 1940's [@Griffin1958;@Dijkgraaf1946], the field of echolocation has moved leaps and bounds. A detailed understanding of the physiology, behaviour, neurobiology and acoustics behind echolocation has been uncovered in great detail [@popper1995hearing;@Fenton2016]. While a large body of knowledge is in place about how *individual bats* are able to echolocate through the use of trained animals in laboratory settings and field observations [@Fenton2016], comparitively little is known on how they echolocate in groups [@ulanovsky2008bat].

Before the digital revolution, the challenges in echolocation fieldwork included bulky analog recording and signal analysis equipment; limiting the speed and quantity of data that could be collected and analysed [@Griffin1958;@habersetzer1981a]. The onset of the digital revolution allowed for lightweight devices that can collect more data, which can then be analysed more easily. Current experimental approaches to study echolocation in groups include, for example, on-body tags that record audio from the focal animal and its neighbours (along with GPS and accelerometer data)[@egert2018resource;@cvikel2015b;@corcoran2021silence], microphone-camera pairs to quantify broad echolocation characteristics with group size [@lin2016bats] and thermal camera 3D tracking with sophisticated computer vision to track individuals in the million-strong emergence behaviour of *Tadarida brasiliensis* [@betke2007tracking;@theriault2010a]. Each of these studies have provided exciting and important insights into how echolocating bats manage to echolocate in groups. For instance, @cvikel2015b showed that *Rhinopoma microphyllum* do not seem to show the 'jamming avoidance response' (shifting call frequencies to avoid  overlap with neighbours, sensu @ulanovsky2004dynamics) first seen by @habersetzer1981a in its congener *R. hardwickei*. @lin2016bats find that *Miniopterus fuliginosus* bats echolocating in caves actually increase their pulse rate with group size, in contrast to theoretical expectations and results from *T. brasiliensis* in captivity which decrease their pulse rate [@jarvis2013groups]. @theriault2010a advanced the field by quantifying the geometry of a *T. brasiliensis* emergence, showing that animals in larger groups fly relatively close to each other (0.5\ m), and that neighbours are placed uniformly across azimuth and elevation around individuals, unlike the elevation-azimuth asymmetry seen in bird flocks [@ballerini2008a]. 

*Ushichka* represents a natural step forward to all the past work in the study of group echolocation in the field. The one important respect where *Ushichka* differs from previous work is its inherent 'multi-modality', in that it consists of simultaneously captured multi-microphone, multi-camera data streams, along with 3D scans of the recording volume for physical context. This multi-modality allows the simultaneous capture of the echolocation and flight behaviour of multiple bat individuals in relation to each other and their surroundings. The acoustic data will provide information about the bats’ sensory strategies to sample their world [@Fenton2016; @hein2020algorithmic]. Furthermore, using acoustic tracking, the audio data provides each bat’s spatial position each time it emits a call [@holderied2003echolocation; @Hugel2017; @surlykke2008echolocating;]. The video data is an independent way to calculate the bat’s 3D positions [@giuggioli2015delayed; @theriault2010a], including also for non-echolocating individuals. In complex audio recordings, position data from video-tracking can also be used for cross-validation with acoustic localisation data. Furthermore, it allows observing non-acoustic behaviour that is not visible in the acoustic data. Lastly, the LIDAR data provides the spatial context of the natural environment, in which the behaviour is happening.   In contrast, on-body tags and single microphone recordings lack explicit information of neighbour location and how many neighbours are there around the focal bat. Pure video tracking with one or more cameras does not provide insights into the bats' echolocation. By capturing both flight and echolocation data of multiple individuals in the group, *Ushichka* promises acce flight and echolocation data of the group, and holds promise to provide access to the auditory scene [@Moss2001] of each individual in the group. Reconstructing individuals’ sensory inputs together with their motor behaviour will offer detailed insights into animals’ sensory-motor algorithms and behavioural decisions [@strandburg2013visual]. Sensory reconstruction is an established method in visually driven animal groups like fish - while auditory scene reconstruction with movement analysis is an exciting new frontier for the field of active sensing collectives. Comparing between sensory systems will address system-specific mechanisms, as well as common functional and ecological principles behind how animals sense and respond to stimuli.

#  Experimental methods

## Field site and study species 
We recorded wild echolocating bats were made in the Orlova Chuka cave system (near Ruse, Bulgaria) in the first chamber  encountered after the entry corridor (Figures \@ref(fig:cavesetup), \@ref(fig:cavesetup2)). Data was collected across 14 recordings sessions (*w x l x h*~5x9x3m$^{3}$) between 19$^{th}$ June to 19$^{th}$ August 2018 (Supplementary Information (SI): Table 6.1). One recording session typically started around sunset and ended around sunrise the next day (see Sec. \@ref(recschedule)).

Orlova Chuka is known to host at least nine species of bats [@Ivanova2005;@govtbatcount]. During the summer, resident  *Myotis myotis* and *Myotis blythii* females together with their offspring form the dominant bat population. The *M. myotis* and *M. blythii* (hereon referred to as *M. myotis/blythii*) female bats arrive in spring and spend the summer in the cave system raising their young. Though rigorous mark-recapture data is missing , individual mothers consistently return to the cave system over multiple nights (pers comm, Laura Stidsholt, Stefan Greif). Given the numerical dominance of  *M. myotis/blythii* in the cave system, it can be fairly said that the *Ushichka* dataset consists mainly of these two species. Since the calls of *M. myotis* and *M. blythii* are indistinguishable (their morphology is also extremely similar too) [@dietz2016bats] we treat them as one group of bats. The other species in the cave stand out in their echolocation calls. Frequency modulating bats such as *M. daubentonii, M. capacinii, Miniopterus schreibersii* have very different call structure, while the constant frequency bats *Rhinolophus ferrumequinum, R.euryale are R. mehelyi* also easily identifiable [@dietz2016bats]. 

```{r echo=FALSE}
# Bat flight and echolocation were recorded in the same recording volume across a period of two/three months with more or less the same recording setup (3 thermal cameras, 12-22 microphones). During both morning and evening recording sessions, bats typically flew a few rounds in the recording volume (*w x l x h*~5x9x3m$^{3}$) before heading back into the direction they came from, or flying into another direction. Recordings were conducted 
```

```{r cavesetup, echo=FALSE, fig.cap='The *Ushichka* setup. The large inverted T shaped array in the centre of the image is the 120cm tristar microphone array with four SANKEN CO-100 microphones. The three blue lights (top-left, bottom-middle, top-right of image) originate from the three thermal cameras pointing towards the array. On the left-hand side of the cave wall, multiple smaller Knowles SPU 0410 microphones are attached to the wall on the left portion of the image (not visible, see  Figure \\@(fig:cavesetup2). Photo by Stefan Greif.'}
cavesetupimg = 'figures/ushichka_setup_DC6A5930_w.JPG'
include_graphics(cavesetupimg)
```

```{r cavesetup2, echo=FALSE, fig.cap=' Multiple Knowles SPU 0410 microphones (at the end of the black cables) were directly mounted on the left-hand side wall in Figure \\@ref(fig:cavesetup). Bats regularly flew by the microphones, seen here: *M. myotis/blythii* . Photo by Stefan Greif.'}
cavesetupimg2 = 'figures/DC6A6061.JPG'
include_graphics(cavesetupimg2)
```

## Audio, video and LiDAR systems 

### Audio  

Multichannel audio data was recorded by multiple (2-3) ASIO-protocol audio-interfaces (Fireface 802 and Fireface UC; RME GmbH, Haimhausen, Germany) running at 192kHz connected to a laptop (DELL Latitude E6540, Windows 7 Enterprise, Intel Core i5, 16GB RAM). The laptop was verified not to emit ultrasound. The number of channels of audio varied between 12-22 channels over the course of the field season. The microphone array consisted of four condenser microphones (CO-100, Sanken, Tokya, Japan) with the rest of the 8-18 channels being MEMS microphones (SPU-0410 Knowles, Itasca, USA). All microphones were connected to either the inbuilt pre-amplifiers on the audio-interfaces, or to two QuadMic II pre-amplifiers (RME GmbH, Haimhausen, Germany) . Later recordings with the *Ushichka* setup also included a Focusrite Scarlett OctoPre (Focusrite Plc, Bucks, UK) pre-amplifier. 

During all recording nights, the four Sanken CO-100 microphones were configured as a 120-cm tristart microphone-array, which was placed in a fairly constant position in the recording volume (front of a memorial plaque on the cave's wall; Figure \ref(fig:cavesetup)). A 'tristar' array is a common planar array configuration [@Hugel2017;@Goerlitz2010;@Lewanzik2018], with one central microphone surrounding by three microphones at a fixed radius and 120$^{\circ}$ angular separation. The Knowles SPU-0410 microphones were mostly mounted on one wall of the recording volume to increase the coverage of recorded echolocations. Microphone positions were constant within a recording session, and were often changed between sessions (except for a few occasions, where microphones were kept in the same position across consecutive nights). Microphones were not left in the cave across multiple days to avoid humidity and moisture droplets affecting the electronic circuits, microphone membranes and sound inlets. 

We used two approaches to measure microphone postions. First, a subset of all possible inter-mic distances were measured every recording session using a laser range finder (1mm accuracy, GLM 50C, Bosch, Gerlingen-Schillerhoehe, Germany). Second, ultrasonic sweeps were played back from multiple points in the recording volume as part of an  automatic mic position estimation workflow. Acoustic calibration of the frequency response and directionality was performed at least once for most of the microphones after the field season. For more information on the microphone automatic position estimation and calibrations please refer to the Section \@ref(noframes) and the SI. 

### Video  

Three thermal cameras (9mm focal length, TeAx ThermalCapture 2.0, TeAx Technology GmbH, Wilnsdorf, Germany) running with FLIR Tau 640 cores (640x512 pixel image resolution) at 25Hz frame-rate formed the camera array. All three cameras (Figure \@ref(fig:multicambats)) were frame synchronised.

```{r multicambats, echo=FALSE, out.width="100%",fig.cap="Thermal video recording. Top: Synchronised frames of the three thermal cameras, recording multiple bats flying the recording volume for subsequent 3D tracking. Bottom: A view of the cameras in the recording volume."}
multicambatsimg = 'figures/multicam_views.png'
include_graphics(multicambatsimg)
```

The thermal cameras recorded video directly onto microSD cards (SAMSUNG PRO+, 32 GB), which were inserted into each camera at the start of each recording session. Since removing the microSD card from the cameras significantly disturbed their original position, the total video recording time was limited by the microSD card memory capacity, and video data was thus downloaded after the end of the recording session. The position and orientation of all cameras were calibrated in the field using the 'wand' protocol of @Theriault2014. For further details on camera intrinsic and extrinsic parameter calibrations, wand setup, and synchronisation with audio - see SI 4. 

### LiDAR 
We performed a LiDAR-scan of the cave surface, along with further large parts of the Orlova CHuka cave (Fig. 2.4). This dataset provides high-resolution (<1 cm) 3D-data of the cave interior as the spatial context for the bats’ observed echolocation and flight behaviour of the bats [@bggeospatial]. Without this environmental data, we will at most be able to recreate each bat’s positions and call timing relative to the other bats. Only after aligning the coordinate systems of the acoustic and video bat data with the cave’s LiDAR data, we will have access to the bats’ absolute position in the cave. Since bats react both to each other’s presence as well as to their physical environment, only such a combined dataset allows a complete understanding of the bats’ sensory-behavioural algorithms, their inter-individual interactions and collective behaviour, and their reaction to their environment. 

```{r lidarimage, echo=FALSE, fig.cap='The LiDAR map of the recording volume showing the same view as in Figure \\@ref(fig:multicambats), with the large tristar array.'}
lidar_scan_img = 'figures/lidar_image.png'
include_graphics(lidar_scan_img)
```

### Weather data
The local weather conditions (relative humidity, temperature and atmospheric pressure) within the cave were recorded continuously by a weather logger (Kestrel 4000 , Nielsen-Kellerman Co., Boothwyn, USA), typically hung from the tristar array stand. Weather conditions determine the speed of sound and the atmospheric attenuation of sound  [@goerlitz2018weather;@lawrence1982measurements], and thus influence the accuracy of acoustically tracked bat positions [@goerlitz2018weather] and source level estimates. 


## Recording schedule {#recschedule}
The audio and video arrays were mostly setup before sunset (~21:00 Eastern European Time) and collected data until around sunrise (~5:30 EET), with one break in between around 12-3 am, which coincided with an observed drop in bat activity. In two of the 14 recording sessions, recordings were triggered manually when bat calls were heard on a bat detector (SI Table 1). The recordings of the remaining sessions were triggered automatically when bat calls on a subset of the monitored channels surpassed a threshold between -50 to -40 dB RMS FS, which was adjusted across between sessions over the field season. Recording were stopped after a pre-defined duration of 10-15 s (variation due to adjustments made over the field season). There were no other ultrasonic sources in the cave, and we are certain that all recordings were triggered by bat calls only. Automatic recording triggering implemented in the ```fieldrecorder_trigger``` module relies on the open-source Python language [@van2011python], and the sounddevice, scipy and numpy [@geier2015a;@virtanen2019a;@oliphant2006a] packages. 

The almost constant bat activity in the cave would continuously trigger our recording system. This would cause unmanageably large data files and fill-up the microSD cards (32 GB) of the TeAX thermal camera system before the end of the night. Thus, we set a duty-cycle for the audio-video recording system (of around 20%), which determined, together with the recording duration, the pause duration between subsequent recordings. For example, a duty cycle of 20% and a recording duration of 10 s meant that recordings could be triggered at most every 50 seconds (10 s of recording followed by 40 s of recording pause). On a few occasions, morning and evening recordings were missed due to various technical and logistical issues.

```{r multichannel-calls, echo=FALSE, fig.cap="Example audio data of bat calls from a subset (four channels from top to bottom) of the multichannel recordings. The spectrograms highlight some of the challenges of working with the multi-bat audio data recorded in a cave, including reverberation, multi-path propagation, call overlaps, and correct call identity matching across channels. A) One bat; note the multiple echoes and reverberation following each call. B) Multiple bats; note additionally the overlapping calls and difficulty of determining identifying the call sequence of a single bat."}
multichcalls = 'figures/single_multibat_specgram.png'# 'figures/multichannel_composite_elongated.png' #'figures/multichannel_composite.png'
include_graphics(multichcalls)
```

## Bat activity
Bats arrived in the recording volume from their further inward-lying roosting sites around sunset. The first activity typically consisted of small groups (N=2-5) of *R. euryale* and *R. mehelyi* flying rapidly and unpredictably in the volume. Subsequently, the number of *M. myotis/blythii* bats slowly increased, reaching up to 30 bats flying simultaneously in the recording volume. The *M. myotis/blythii* bats also started to form temporary roosting clusters on the walls of the recording volume towards end of June, yet with decreasing frequency as the season progressed further. These temporary roosts lasted for a few hours from sundown until midnight, after which they dispersed. During cloudy and rainy evenings, the emergence from the cave was delayed or ceased completely. 

Peak bat activity in the recording volume was typically seen about 1-1.5 hours after sun-set, after which bats began to exit the cave (Figure \@ref(fig:activity)). The exit happened relatively rapid, after which only a few bats were seen to still fly in the recording volume. Individual bats returned in the early morning from one to two hours before sunset onwards. It seemed that *R. euryale* and *R. mehelyi* activity remained fairly constant through the night, even during the drop in general activity around midnight. 

```{r activity, echo=FALSE, fig.cap='Temporal pattern of bat activity. Schematic of typical bat activity pattern of Myotis myotis and Myotis blythii between sunset to sunrise in the *Ushichka* dataset. The multichannel recording system was typically activated before sunset and was run till after sunrise with a break in between'}
activityimg= 'figures/ushichka_activity.png'
include_graphics(activityimg)
```

# Technical challenges ahead and potential solutions in analysing the *Ushichka* dataset

To realize the true potential of the Ushicka dataset for studying echolocation and flight in groups, we will need to surmount a series of challenges related to analysing multi-call echolocation data. Here they are discussed in increasing order of difficulty.

1) **Sound reflections** : The cave walls surrounding the recording volume are strong acoustic reflectors, leading to significant multi-path propagation. Multi-path propagation occurs when a sound arrives multiple times at a microphone due to multiple distinct reflections at different surfaces. For instance, a specific call arrives first at the microphone via its direct path from the bat to the microphone. It is then followed by the multiple reflections from the ground, the cave walls and the ceiling at different delays depending on the total travelled distance. With increasing number of potential reflectors and bats, multi-path propagation will become increasingly difficult, since N reflectors and M emitted calls could cause up to NxM recorded call reflections. Acoustic tracking of bat position depends on calculating the time-difference-of-arrival different (TDOA) of the same call at the different microphones, typically via cross-correlation of the audio channels. The many false ‘copies’ of the original call within each channel can create spurious peaks in the cross-correlation, causing wrong TDOA estimates and thus location errors. Solutions to handling multi-path propagation exist, such as the DATEMM algorithm and its variants  [@kreissig2013fast;@scheuing2008disambiguation;@zannini2010improved].For the example the zero-sum  TDOA criterion for one sound recorded on three channels is used to identify reliable TDOA measurements in @kreissig2013fast - followed by a 'graph-synthesis' approach to genereate a full set of TDOAs for source localisation.

2) **Call density** : Each bat flying in and out of the volume emits about 10-20 calls per second (10-20 Hz call rate). The acoustic localization of one bat is relatively straightforward since calls are separated by tens of milliseconds. With increasing number of bats, the temporal separation between individual calls decreases and the probability of calls overlapping in time increases. Analysing calls at such high densities is challenging and will require the latest techniques for call detection in the presence of overlaps  [@izadi2020separation], or in general blind signal separation techniques [@brandstein2013microphone]. 

3) **Correspondence matching** : In addition to reliably detecting individuals calls in single channels, the same calls must be identified across channels, a common problem in any multi-channel acoustic array [@brandstein2013microphone]. Specifically, an echolocation call detected in one channel needs to be matched to itself in another channel, despite the possible presence of multiple similar calls in the expected time-window. Individual bats may have unique call characteristics (@masters1995sonar, and in *M. myotis*: @yovel2009voice; but also see @siemersnovoice).However, since manual correspondence matching is prohibitive for larger group sizes and numbers of calls to be analysed, robust automated algorithms are needed to solve this problem. Furthermore, echolocation calls have very flexible acoustic characteristics, which bats constantly adapt to the current task [@Goerlitz2010;@lewanzik2021task;@Fenton2016], and which may override potential individual-specific acoustic signatures, rendering bat echolocation calls difficult for call classification. DATEMM-type algorithms [@kreissig2013fast;@scheuing2008disambiguation] may present a solution. Much like multi-path propagation, the presence of overlapping sounds also creates spurious peaks in cross-correlations between two channels. For example @kreissig2013fast is able to handle and localise overlapping sounds by filtering cross-correlation peaks and TDOA combinations based on the assumptions that follow from sources that have direct path propagation. While the output of a DATEMM type localisation is the 3D positions of the sources, these sources correspond to a set of TDOAs across channels. Knowing the candidate TDOAs may allow post-hoc matching of the emitted calls across channels. 

Based on the multi-channel acoustic data recorded by multiple microphones at known positions, the position of vocalising animals can be calculated for each received call (“acoustic tracking”, @aubauer;@aubauerRuppert). The basic principle of acoustic tracking relies on the fact that sound travels at a limited speed and thus arrives at different times at different positions in in space. When comparing the sound arrival time between at least four microphones, each position of sound emission will generate a unique set of TDOAs between those microphones. Measuring TDOAs thus allows to calculate the position at which a bat emitted a call. With increasing number of bats and reflections, acoustic tracking will become increasingly challenging, as discussed above. Our multi-camera video data will provide one important ‘support system’ to solve the issues of multi-path propagation and correspondence matching. The time-synchronised video tracking data provides an independent estimate of the 3D-positions of the microphones and bats. By combining the audio and video data, we can simplify cross-channel correspondence matching and the process of assigning a call to its source bat. Each call is emitted while the bat is at one position in space. The bat’s position is recorded on video, while the call is recorded on multiple audio channels, resulting in one correct set of TDOAs for this spatial position. The video-observed bat positions represent all possible points of call emission over time. Based on the bat-microphone distances, we can generate a set of ‘predicted’ TDOAs between microphone pairs over time. The problem thus boils down to a matching problem between measured and predicted TOADs. Given a set of video-based bat flight trajectories, we are able to filter out a larger number of spurious TDOAs. The trajectory based filtering approach could in principle elegantly solve the problems of multi-path propagation and cross-channel correspondence at one shot. It still remains to be tested, however, whether the video-based 3D-positions are of sufficient accuracy to provide sufficiently accurate TDOA predictions.  

## Fusing multiple data streams to reconstruct individual auditory scenes
Reconstructing individual auditory scenes requires recreating the sounds received by a bat, such as its own echoes, the calls of neighbours, and the echoes of their calls. Recreating the incoming sounds heard by a bat thus requires knowing the relative positions of the bats in the group, their locations in the recording volume, and the timing of their call emissions. The acoustic data from the microphone array allows to calculate the timing and spatial position of each recorded call and to reconstruct the flight trajectories of calling individuals. Based on this information, the calls received by each bat can be calculated. Second, the echoes received by each bat will have reflected come from objects in each surroundings, most notably the other (moving) bats and the (stationary) walls of the cave. To reconstruct these echoes, we require information about the bats’ spatial position (e.g., via acoustic tracking and/or video tracking) and the cave walls (via the LIDAR scan). Reconstructing the auditory scenes of the bats in the group thus requires a fusion of the various data streams in the *Ushichka* dataset, including aligning these data into a common coordinate system.

The audio and video streams are tightly linked, because they record the flight behaviour of the same bats in the recording volume. Fusing the acoustic and video data streams is relatively straightforward because they are time-synchronised and they both track common ‘objects’ (bats). Thus, the 3D positions derived from both data streams can be overlaid to match with a simple rigid rotation of coordinate systems. However, to understand the bats’ echolocation and flight in relation to their physical environment, we need to align the LiDAR data of the cave with the coordinate systems of the flight trajectories derived from acoustic and video tracking. Recent advances in the alignment of LiDAR scans with camera views have led to the formulation of ‘targetless’ workflows that do not require specific calibration objects for alignment [@kang2020automatic;@munozbanon2020]. These approaches recreate synthetic camera images by projecting the 3D LiDAR scan into 2D (e.g., Figure 2.3) and then search for the solution with the highest similarity between the 2D-projection and the camera image (or some transformation of the images). These alignment methods can be applied to the *Ushichka * dataset, since we measured the extrinsic (position and rotation) and intrinsic (eg. focal length, distortion) camera parameters that are required for 2D projection. One potential issue, however, might be that both approaches have been developed for visible light cameras. Thermal images, however, often look very different to visible light images, and thus may require different or additional processing steps to match the LIDAR-based 2D-projection. To our limited knowledge, current thermal camera-LiDAR alignment protocols require a specifically designed calibration object [@krishnan2017cross; @zhangcalib;@slatcalib], and there is still limited work on target-less thermal camera-LIDAR alignment [@phuc2017registration]. For *Ushichka*, the low dynamic range of the thermal images due to the relatively stable temperature in the cave (~10°C) may present an additional challenge. Spatial features of the cave will thus not show up on thermal images, if the spatial features do not correspond to differences in temperature. Given these challenges, we have developed a semi-automatic target-free method to align the video and LiDAR scenes in *Uschichka* [@DMCP]. As it stands, we have been able to achieve $\leq 5$ pixels 2D reprojection error, and $\leq 0.5 m$ 3D alignment error, with scope for improvement considering the challenging dataset.  

Combining the acoustic and video data on the flight and echolocation behaviour of multiple bats with the LIDAR data on the bats’ environment will allow unprecedented insights into the sensory biology, movement ecology and inter-individual interactions of free-flying active sensing animals. We will be able to reconstruct sensory information and interpret the animal’s decisions in their social and spatial context.

## Automatic mic position estimation removes the need for bulky frames {#noframes}

For successful and precise acoustic localisation, the (relative) microphone positions must be known [@Wahlberg1999]. Acoustic tracking thus typically relies on rigid frames and tripods to hold microphones in known positions. In the field, especially in cluttered environments such as caves or forests, working with large array frames can however be difficult and time-consuming. Furthermore, bats may react to these novel conspicuous objects by changing their calling behaviour, making the recorded calls an unwanted product of the setup itself. There is thus a pressing need in bioacoustics for the development of acoustic arrays that are less conspicuous, and can be setup universally and quickly. One option, is to place the microphones on existing structures such as the cave walls here in Ushichka; alternative pre-existing structures at other field sites may be rocks, trees, branches, signposts etc. Using pre-existing structures avoids generating additional obstacles and new points of interest, and is thus less likely to generate artifactual inspection behaviours by the bats.

Once rigid frames have been abandoned in favour of pre-existing structures, the (relative) microphone positions must be measured. This can directly be done with a Total station theodolite, which directly outputs the microphones 3D positions of microphones with high mm-precision. The down-side of this approach is the need for another (expensive) piece of equipment and the required time to measure each microphone. A cheaper, yet even more time-consuming alternative is to exhaustively measure inter-microphone distances manually using a laser range finder (as we partially did for Ushichka) or tape-measure. However, manual measurements with laser range finders can become error-prone at larger distances due to jitter (particularly when measured on continuous surfaces such as cave walls), and will become impractical with tape-measures. Furthermore, manual inter-mic distance measurements require direct line-of-sight between microphones and increase rapidly with increasing number of used microphones. 

In contrast to these manual approaches to microphone positioning, modern camera calibration workflows are much more automatized [eg. @Theriault2014]. For microphones, the ‘structure-from-sound’ approach [@zhayida2016automatic] provides a similar automatized workflow, which infers microphone positions based only on commonly recorded playbacks, without the need for any measurements of microphone positions or distances. The structure-from-sound approach is able to robustly localize microphones even in our echoic cave environment [@sfs_cotdoa, also see SI]. This drastically reduces the required effort for multi-channel sound recordings in the field, enabling the exciting realm of animal observations in the field based on inconspicuous experimental setups. Our ongoing technological developments aim to provide field-friendly microphone position estimation workflows, that are widely applicable across diverse field-sites.


```{r echo=FALSE}
# Implemented all HRG suggested changes till here 13/12/2022 -15:03
```

# Investigative opportunities opened by the *Ushichka* dataset

Here we detail some of the possible lines of investigations that can be undertaken with *Ushichka* once the necessary analytical workflows are in place. 

## Collective behaviour in active sensing groups 
The combination of acoustic and video tracking along with a LiDAR scan of the cave provides us sufficient data to reconstruct the sensory inputs of each bat in a group. To reconstruct the sensory inputs of each bat, we would require baseline data on a series of parameters such as call and hearing directionality, auditory masking, hearing threshold and source level. These parameters can either be extracted from the literature (as done by @beleyur2019modeling and @mazar2020sensorimotor) or measured using the acoustic tracking system itself (eg. source level and call directionality). Given a group of *N* bats, we can thus reconstruct all the sounds each bat would have heard in their inter-pulse intervals (the silent gap between calls). Previous studies have attempted to simulate such contexts with biologically parametrised values of the input sounds [@beleyur2019modeling;@mazar2020sensorimotor]. Our experimental measurements in combination with detailed sound propagation models could allow for *direct* sensory input reconstructions in a whole group of bats simultaneously. Knowing the sensory inputs animals receive over time would allow us to finely parametrise the relevant models of collective behaviour and test how well they predict group  motion. For instance, @bode2011a model a group of agents that perform non-simultaneous and discontinuous 'updates' of their environment. Their model is very similar to how groups of bats get information about their surroundings, as each bat is very likely emitting calls independently [@hase2018a]. @bode2011a moreover model 'limited-interactions' in collective motion, where each agent can only detect the positions of one neighbour  at each update, very similar to what bats may be experiencing in large groups [@beleyur2019modeling]. Using the 'limited-interactions' model, @bode2011a are able to show that their predictions match empirical observations of starling flock structure. More importantly, @bode2011a also observe that their 'limited-interactions' model results in quantitatively different group structure in comparison to commonly used zone-based models [eg. @couzin2002a]of collective behaviour. The @bode2011a model can be parametrised by estimating how often bats call in a group (update rate), and how many neighbours they are typically able to detect (eg. using the framework in @beleyur2019modeling). The motion of *in silico* active sensing groups can then be compared with observed bat groups. Sensory reconstruction approaches in echolocation would also open the field to fine-scale temporal analyses of echolocation and flight behaviour: do bats turn away more often in response to an echo from a wall, or from the direction of a calling neighbour? Do bats call more often when they may have 'missed' echo detections due to call overlaps in the past few interpulse intervals?

## Optimising echolocation and flight strategies under challenging conditions
Bats are known to alter their echolocation rapidly according to the behavioural context, and ambient soundscape [@corcoran2017sensing]. *Ushichka* offers direct access to bat echolocation strategies as they are  acoustically challenged by increasing group sizes (Fig \@ref(fig:activity)). Aside from call-level data such as source-level, duration and spectral properties - the presence of multiple microphones spread across the room also provides access to call direction and beam shape modulations (Figure \@ref(fig:beamshape)A). While other call parameters have been investigated in studies with multiple animals, call directionality and beam-shape measurements have so far mainly been done with single animals in the field and lab [@surlykke2012a;@giuggioli2015a]. Revealing where bats aim their calls, and how they spread the energy in each call will uncover the sensory priorities bats may have while flying under challenging conditions. Do bats choose to focus narrowly on fast moving conspecifics, but broadly onto bigger obstructions like walls? Does their call direction and beam shape change with increasing group size? 

```{r beamshape, echo=FALSE, fig.cap="The benefits of multi-mic arrays for the study of echolocation and voice recognition (microphone: blue circle with line attached) A) Biosonar beam-shape can be estimated from microphones spread around the animals B,C) As the bat moves through the volume (C 1-8), the amplitude and spectral properties of the call received by the focal microphone (red) changes, as seen on the spectrogram (B). Having multiple microphones provides a multi-view picture of a call from many directions, allowing a better assessment of intra-individual call variation required for robust voice recognition"}
beamshapeimg = 'figures/beamshape_cleaned.png'
include_graphics(beamshapeimg)
```

The echolocation strategies in use may also inform why certain types of apparently stereotypical flight paths are seen within the recording volume. It was observed that bats sometimes tend to fly a circular loop close to the edges of the recording volume before leaving the cave or heading back into the roosting site (Figure \@ref(fig:loopingsch)). This 'looping' flight behaviour was observed in the evening flight activity as bats began to accumulate in the recording volume, but also when bats returned alone in the morning. Bats are known to show  stereotypic flight behaviours in the lab and in the wild [@mohres1949versuche;@barchi2013spatial], and this may perhaps be due to their limited attention or spatial memory.  However, in a cave setting, where there are no obvious obstacles, what drives the formation of these looping behaviours? Are the loops a stereotypic trajectory that allows the bat to limit its attention only along that route? Why do bats not take the shortest path and fly from the exit straight into roosting site? Initial observations of group flights suggested that even when multiple bats were flying together, not all parts of the recording volume were being 'used' by the bats. Some regions of the recording volume (regions close to walls)  seemed to have a higher density of bats. Is this wall-clinging behaviour a behavioural adaptation to maintain a high received level of wall-echoes (in the presence of non-target sounds)? The wall-clinging behaviour is somewhat reminiscent of structure-following behaviour seen in the field with horsehoe bats, that follow hedgerows instead of taking shortcuts over open fields. Horseshoe bats however, are likely to follow structures due to the limited sensing range from their high-frequency echolocation - why would myotids also adopt this behaviour? Another interesting parameter to estimate is the dynamics of the loop's chirality (clockwise/counter-clockwise) with group size. Alignment is a standard measure of how similar animals are in their direction of movement. When there are multiple animals in the recording volume, a measure of the alignment over time and group size might reveal whether the looping behaviour is an individually driven behaviour or is modulated by the flight directions of group members. 


```{r loopingsch, echo=FALSE, fig.cap="Schematic showing a view of the direct and looped flight paths as seen from above. The direct (broken blue line) and indirect (continuous green line) paths are flown in both directions (exit/entrance-cave interior, cave interior-exit/entrance). The grey quadrilateral outlines the recording volume."}
loopingimg = 'figures/looping.png'
include_graphics(loopingimg)
```

## Voice recognition for short-term and long-term behavioural tracking 
Many vocalising vertebrates across the animal kingdom have individually identifiable vocalisations, or 'voices' [@carlson2020individual]. The voice of each animal is a temporally stable cue, unlike experimental markings which may be lost over time. Despite the known stability of voice, for some animals one of the main challenges with developing a stable voice identifier is the variation in the vocalisation itself. Bird call repertoires can change over time, and complicate identification [@stowell2019automatic]. Approaches to date rely on machine-learning and statistical models, which require a training dataset of example vocalisations to 'learn' the patterns in the data. Variation caused by sound directionality, distance and ambient noise may often necessitate larger training datasets, which are often hard to obtain [@stowell2019automatic]. Unlike many types of animal vocalisations (eg. bird song, mammal social vocalisations), the spectro-temporal structure of individual bat echolocation calls are stereotypical and likely to remain stable because they serve a single sensory purpose – to detect objects. The dedicated sensory functionality of echolocation calls, and the general absence of background ultrasonic noise in the wild makes bat calls a convenient system to develop voice identification workflows.

Bat echolocation calls can be assigned to individuals by their acoustic features, though this has typically been done in small lab populations. Previous work [@masters1995sonar;@yovel2009voice] solved the 'closed-set' problem where all animal identities are known. *Ushichka* provides a large dataset over multiple nights and animals that fly by [also see Figure \@ref(fig:beamshape)B,C]. In contrast to the 'closed-set' problems solved in the past, *Ushichka* could be used to generate methods to solve the 'open-set' voice recognition task. Open-set classification problems deal with models that uniquely identify individuals that they have been trained on, but also those they have *not* been explicitly trained on. Even though the identities of bats echolocating in the recordings are unknown, it may be possible to exploit the behavioural patterns of the animals to approximate identity. Bats always return alone in the morning, and these recordings contain multiple echolocation calls from one individual as it enters the recording volume and heads away to the roosting site. In addition, it is known that a stable population of resident female *M. myotis/blythii* roost in the cave system over the entire summer period. Using these two facts, if successful, a voice recognition model should at least show certain patterns:  1) correctly assign identity to an independent subset of calls it was not trained on (validation data) from morning recordings 2) identify at least some individuals consistently across multiple morning returns, and in the best case 3) be able to pick up the individuals as they begin their activity in the recording volume around sunset. Of course, even if a trained model satisfies the three criteria above, it does not directly follow that the detections in scenarios 2) and 3) are not false positives. It may be necessary to systematically collect a larger sample of echolocation calls from hand-released individuals. The state of signal-processing and machine learning know-how in the field of human voice recognition is relatively mature at this point, with the use of speaker-specific Gaussian Mixture Models compared to Universal Background Models trained against multiple individuals [@Hennebert2009]. *Ushichka* may provide a test-dataset to examine how well the latest voice recognition approaches perform for echolocation calls. 

While seemingly challenging, if a validated voice recognition model is indeed developed successfully, it will allow revolutionary markerless short-term and long-term tracking of individual behaviours. Short-term behaviours such as flight and echolocation decisions over the course of a few seconds can provide insights into  individual sensorimotor strategies and the tracking of social interactions. Long-term decisions such as the change of evening cave exit-time and morning return times can reveal the changing physiological status of the resident females as their pups mature. Voice based individual recognition brings the same advantages gained by  tracking mammals  with unique visible markings (eg. zebras, tigers) [@lahiri11_biometric;@hiby2009tiger], allowing unobtrusive observations over multiple time-points and scales.

# Supplementary code and website

The list of supplementary code, data and information are below:

#. The ```fieldrecorder_trigger``` module used to trigger automatic recording is available at the following repository:[https://github.com/thejasvibr/fieldrecorder](https://github.com/thejasvibr/fieldrecorder).

#. The speaker playback data to benchmark automatic microphone self-positioning is available at: [https://doi.org/10.5281/zenodo.5126695](https://doi.org/10.5281/zenodo.5126695)

#.More snippets of the the *Ushichka* dataset and research progress is documented at:  [https://thejasvibr.github.io/ushichka/](https://thejasvibr.github.io/ushichka/).

# Field work permits 
All field work at the Orlova Chuka cave was performed under  license of the relevant local authorities (Permit # 795/17.05.2019). Ushichka is a purely observational dataset, no animals were handled or subjected to experimental treatments of any sort during data collection. 

# Author Contributions
TB: conception, experiment design, field data collection, lead writing. HRG: conception, experiment and recording-system design, writing.

# Acknowledgements
We to thank Antoniya Hubancheva, for her constant help and support in the collection of the *Ushichka* dataset (also for christening the dataset with its Bulgarian name) and the Tabachka field crew of 2018. The electronics (Markus Abels, Hannes Sagunsky, Reinhard Biller) and Feinmechanik (Erich Koch, Felix Hartl, Klaus Pichler) teams were of invaluable help and support throughout the design and troubleshooting process of the experimental setup. We furthermore thank Pranav Khandelwal for extensive help setting up the thermal camera calibration workflow and Hedrick Tyson for providing helpful feedback and estimating camera parameters. Fieldwork-wise, we are grateful to Joanna Furmankiewicz for supporting initial recordings at the Jaskinia Niedźwiedzia and Diane Theriault for providing advice on thermal cameras. Special thanks and acknowledgement to the field assistance of Aditya Krishna and Neetash Mysuru, without whom the data collection would not have been possible. T.B. was funded by a GSSP doctoral fellowship from the German Academic Exchange Service (DAAD) and the International Max Planck Research School for Organismal Biology. H.R.G. was funded by the Emmy Noether program of the German Research Foundation (grant no. 241711556DFG). We  thank the members of the Acoustic and Functional Ecology groups and the Max Planck Institute for Ornithology, Seewiesen for its support and infrastructure. 

# References